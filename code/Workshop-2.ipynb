{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../code/Resources/cropped-SummerWorkshop_Header.png\"> \n",
    "\n",
    "<h1 align=\"center\">Workshop 2: Behavioral state identification </h1> \n",
    "<h3 align=\"center\">Summer Workshop on the Dynamic Brain</h3> \n",
    "<h3 align=\"center\">Thursday, August 28th, 2025</h3> \n",
    "<h4 align=\"center\">Day 4</h4> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p> During the morning lecture, we learned about behavioral states and how they affect the processing of sensory stimuli and shape actions. In this workshop, we will explore how to define behavioral states using computational methods. We'll explore the Visual Behavior Neuropixels dataset as an example case. We'll first use data visualization to gain an intuition for task-engaged versus task disengaged behavioral states in this dataset. Next, we'll learn about Hidden Markov Models and use them to identify behavioral states in an unsupervised manner. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os  \n",
    "\n",
    "# Data handling packages\n",
    "import numpy as np  \n",
    "import numpy.random as npr  \n",
    "import pandas as pd \n",
    "import pynwb  \n",
    "import random\n",
    "\n",
    "# Progress bar utility\n",
    "from tqdm import tqdm  # Displays a smart progress bar during loops\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler  # Standardizes features (zero mean, unit variance)\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt  \n",
    "from matplotlib import colors  \n",
    "import seaborn as sns  \n",
    "\n",
    "# Pandas display settings\n",
    "pd.set_option('display.max_columns', None)  # Ensures all columns are shown when printing DataFrames\n",
    "\n",
    "# Inline plotting for Jupyter Notebooks\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Visual Behavior experiment from NWB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div style=\"border-left: 3px solid #000;  border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">   \n",
    "<p> To begin, let's start by reacquainting ourselves with the Visual Behavior dataset from Workshop 1. However, this time we will explore their behavioral patterns.\n",
    "\n",
    "<p> Here's a link to the Visual Behavior Neuropixels Databook: </p>\n",
    "\n",
    "[VBN Databook](https://allenswdb.github.io/physiology/ephys/visual-behavior/VB-Neuropixels.html)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sessions we'll use in this tutorial\n",
    "example_session_ids = [1139846596, 1152811536]\n",
    "\n",
    "session_id = example_session_ids[0] # Let's look at the first example session\n",
    "nwb_path = f'/root/capsule/data/visual-behavior-neuropixels/behavior_ecephys_sessions/{session_id}/ecephys_session_{session_id}.nwb'\n",
    "\n",
    "# access the session data with pynwb\n",
    "session = pynwb.NWBHDF5IO(nwb_path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick reference of the NWB file structure\n",
    "# Important groups include: units, trials, intervals, and processing \n",
    "session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trials dataframe and look at a few rows\n",
    "trials = session.trials[:]\n",
    "trials.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mouse behavioral performance over the session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000;  border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "How well does the mouse identify image changes? We can quantify their performance by evaluating the fraction of 'go' trials during which the mouse licked the spout ('hit' trials).      \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_go_trials = len(np.where(trials.go)[0])\n",
    "num_go_hit_trials = len(np.where(trials.go[trials.hit])[0])\n",
    "print('Proportion of trials correctly performed: ' + str(np.round(num_go_hit_trials/num_go_trials, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: rgb(32, 177, 13); border-radius: 3px; padding: 10px; color: white;\"> \n",
    "<p><b>Task 2.1:</b>  Examine the 2nd example mouse. Print the proportion of 'go' trials that the mouse performed correctly and evaluate their mean performance. (Note: session = get_session(session_id) will take a minute or two to load the session))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:rgb(240, 230, 216); border-radius: 3px; padding: 10px;\">\n",
    "Note: Please be wary while naming variables. Do not use variable names that are already assigned. Preferably, append the number corresponding to the example you are analyzing as we have done below. For example, if analyzing example mouse 2, use num_go_trials_2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank coding cell to work out Task 2.1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000;  border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Looks like these mice fail to respond in about 40-50% of the trials!\n",
    "\n",
    "<p> Next, we will plot licking patterns for each 'go' trial to get a sense of when the mouse fails to respond.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 'go' trials from trials table\n",
    "go_trials = trials[trials.go]\n",
    "\n",
    "# Get lick times for each 'go' trial. Note: we subtract the time of the image change\n",
    "lick_times = go_trials.lick_times.values - go_trials.change_time_no_display_delay.values\n",
    "\n",
    "# Plot with eventplot\n",
    "fig, ax = plt.subplots(figsize=(4, 5))\n",
    "ax.eventplot(lick_times, orientation='horizontal', colors='black', linelengths=1, lineoffsets = go_trials.index)\n",
    "\n",
    "# formatting \n",
    "ax.set_xlabel('Time from image change (s)')\n",
    "ax.set_ylabel('Trial #')\n",
    "ax.set_title('Lick Raster')\n",
    "ax.set_ylim(-1, go_trials.index[-1]+4)\n",
    "ax.set_xlim(-1, 6)\n",
    "plt.gca().invert_yaxis()  # Trial 0 at top\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000;  border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Interestingly, this mouse licks on many trials during the first part of the session but then rarely licks during the later trials.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: rgb(32, 177, 13); border-radius: 3px; padding: 10px; color: white;\"> \n",
    "<p><b>Task 2.2:</b> Plot lick rasters for the other example mouse. Does this mouse show a trend in licking behavior over the session?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank coding cell to work out Task 2.2  \n",
    "# hint: use \"trials_2\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> So, these mice seem to stop responding during the later trials in the session. Another way of looking at this data is to quantify the number of licks in each trial. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying task-engagement states based on visual inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefine figure\n",
    "fig, ax = plt.subplots(figsize = (8, 3))\n",
    "\n",
    "# Get lick counts for each go trial\n",
    "lick_count = go_trials.apply(lambda row : len(row['lick_times']), axis = 1)\n",
    "\n",
    "# Plot lick count\n",
    "ax.plot(lick_count, color='k', lw = 1.5)\n",
    "\n",
    "# Formatting\n",
    "ax.set_ylabel('licks per trial')\n",
    "ax.set_xlabel('trials')\n",
    "ax.set_xlim(go_trials.index[0], go_trials.index[-1])\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Could the mouse be disengaging from the task as they reach the end of the session? The reduction in licking seems to indicate this. \n",
    "<p> We can quantify their performance by computing the hit rate over the behavioral session. Hit rate is the probability the mouse correctly performs a lick during 'go' trials. Here, we will evaluate this as the fraction over 10 go-trials.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hit rate with 3 trial rolling window.\n",
    "hit_rate = go_trials.hit.rolling(3).mean()\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (8, 3)) \n",
    "ax.plot(go_trials.index, hit_rate, color = 'k', lw = 1.5)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlim(go_trials.index[0], go_trials.index[-1])\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.set_xlabel('trials')\n",
    "ax.set_ylabel('hit rate')\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> We see that the hit rate is very high during the first 400 trials, then drops to zero around trial 500. Thus, the mouse is clearly \"task engaged\" during the first part of the session, but then becomes \"task disengaged\". Using this insight we can set an  threshold on hit rate to define engaged and disengaged states. What's a good threshold to set? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for defining engageded vs disengaged states\n",
    "engagement_threshold = 0.2\n",
    "\n",
    "# Plot hit rate and line for engagement threshold\n",
    "fig,ax = plt.subplots(figsize = (8, 3)) \n",
    "ax.plot(go_trials.index, hit_rate, color = 'k', lw = 2)\n",
    "ax.plot(go_trials.index, engagement_threshold*np.ones(num_go_trials), color = 'r', ls = '--')\n",
    "\n",
    "# Determine state transition boundaries\n",
    "states = hit_rate < engagement_threshold\n",
    "ax.fill_between(states.index, -0.1, 1.1, where = states == 1, alpha=0.5, color = 'tab:green', label = 'state 0')\n",
    "ax.fill_between(states.index, -0.1, 1.1, where = states == 0, alpha=0.5, color = 'gold', label = 'state 1')\n",
    "\n",
    "# Formatting\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "ax.set_xlim(go_trials.index[0], go_trials.index[-1])\n",
    "ax.set_xlabel('Trials')\n",
    "ax.set_ylabel('Hit rate')\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), bbox_to_anchor = (1.01, 1))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do running speed and pupil size vary with task-engagement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> When the mouse’s task-engagement state changes, does it affect other aspects of its behavior too? To answer this, let’s look beyond their lick patterns and examine additional variables including running speed and pupil size. </p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamps corresponding to go trials\n",
    "trial_start = go_trials.start_time\n",
    "trial_stop = go_trials.stop_time\n",
    "\n",
    "# initialize data dictionary\n",
    "behavior_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get running speed and corresponding timestamps\n",
    "running_data = session.processing['running']\n",
    "running_timestamps = running_data['speed'].timestamps[:]\n",
    "running_speed = running_data['speed'].data[:]\n",
    "running_speed = pd.Series(running_speed).interpolate(limit_direction='both').to_numpy() \n",
    "behavior_data['running_speed']  = [np.nanmean(running_speed[np.logical_and(s1 <= running_timestamps, running_timestamps <= s2)]) for s1, s2 in zip(trial_start, trial_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pupil area and corresponding timestamps\n",
    "pupil_data = session.acquisition['EyeTracking']['pupil_tracking']\n",
    "pupil_timestamps = pupil_data.timestamps[:]\n",
    "pupil_area = pupil_data.area[:]\n",
    "pupil_area = pd.Series(pupil_area).interpolate(limit_direction='both').to_numpy() \n",
    "behavior_data['pupil_area'] = [np.nanmean(pupil_area[np.logical_and(s1 <= pupil_timestamps, pupil_timestamps <= s2)]) for s1, s2 in zip(trial_start, trial_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lick count data to behavior_data dictionary\n",
    "behavior_data['lick_count'] = lick_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_plot = ['running_speed', 'pupil_area','lick_count']\n",
    "\n",
    "fig, ax = plt.subplots(len(keys_to_plot), 1, figsize = (8, 5), sharex = True)\n",
    "\n",
    "for i, key in enumerate(keys_to_plot):\n",
    "    ax[i].plot(go_trials.index, behavior_data[key], color = 'k', lw = 1)\n",
    "    ax[i].set_title(key, fontsize = 10)\n",
    "    \n",
    "    ax[i].spines[\"top\"].set_visible(False)\n",
    "    ax[i].spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # Get min and max of this behavior trace\n",
    "    min_val = np.nanmin(behavior_data[key])\n",
    "    max_val = np.nanmax(behavior_data[key])\n",
    "\n",
    "    # Fill between min*vis_context and max*vis_context\n",
    "    ax[i].fill_between(states.index, min_val, max_val, where = states == 1, alpha=0.5, color = 'tab:green')\n",
    "    ax[i].fill_between(states.index, min_val, max_val, where = states == 0, alpha=0.5, color = 'gold')\n",
    "\n",
    "    ax[i].set_xlim(go_trials.index[0], go_trials.index[-1])\n",
    "\n",
    "fig.text(0.5, 0.00, 'Trials', ha='center')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> So, while defining behavioral states by thresholding the hit rate is a useful starting point, it is incomplete. There are several additional variables that could provide more nuanced insights into the mouse’s behavioral states but we have not incorporated these into our definition of behavioral states. </p>\n",
    "\n",
    "<p>Moreover, the threshold we used to define behavioral states appeared appropriate for the mouse we studied but may not generalize across animals. How, then, can we systematically assess state changes across many mice?</p>\n",
    "\n",
    "<p>In the remainder of this workshop, we will explore more sophisticated methods for defining behavioral states that integrate multiple features to provide a richer, more reliable description of each mouse’s behavioral profile.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine covariation between behavioral features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<p> First, we'll learn how to make sense of a more complex view of the behavioral information. Instead of looking at just one behavioral variable, we will examine multiple variables simultaneously to gain a better understanding of what's going on. As we have seen, some of these behavior variables correlate with the task-engaged behavioral state and also correlate with each other. Let's create a visualization to see how the different behavioral variables are correlated.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert behavior_data dictionary to dataframe for ease of use \n",
    "behavior_df = pd.DataFrame(behavior_data)\n",
    "behavior_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set layout for pairwise plot - 3 X 3 plot grid\n",
    "g = sns.PairGrid(behavior_df,  vars=['running_speed', 'pupil_area', 'lick_count'], diag_sharey=False)\n",
    "\n",
    "# Plot 2D density plot in the lower triangle \n",
    "g.map_lower(sns.scatterplot, s=15, alpha=0.5, linewidth=0)\n",
    "\n",
    "# Hide the upper triangle \n",
    "def hide_current_axis(*args, **kwds):\n",
    "    # function to hide upper triangle of the pairwise plots\n",
    "    plt.gca().set_visible(False)\n",
    "g.map_upper(hide_current_axis)\n",
    "\n",
    "# Plot 1D density plot\n",
    "g.map_diag(sns.kdeplot, hue=None, legend=False, bw_method = 'scott')\n",
    "\n",
    "# Formatting \n",
    "g.fig.set_size_inches(6,6)\n",
    "g.fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set layout for pairwise plot - 3 X 3 plot grid\n",
    "g = sns.PairGrid(behavior_df,  vars=['running_speed', 'pupil_area', 'lick_count'], diag_sharey=False)\n",
    "\n",
    "# Plot 2D density plot in the lower triangle \n",
    "width = 'scott' # \"Scott Method\" for choosing width of density kernel\n",
    "g.map_lower(sns.kdeplot, hue=None, bw_method = width)\n",
    "\n",
    "# Hide the upper triangle \n",
    "def hide_current_axis(*args, **kwds):\n",
    "    # function to hide upper triangle of the pairwise plots\n",
    "    plt.gca().set_visible(False)\n",
    "g.map_upper(hide_current_axis)\n",
    "\n",
    "# Plot 1D density plot\n",
    "g.map_diag(sns.kdeplot, hue=None, legend=False, bw_method = width)\n",
    "\n",
    "# Formatting \n",
    "g.fig.set_size_inches(7,7)\n",
    "g.fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: rgb(32, 177, 13); border-radius: 3px; padding: 10px; color: white;\"> \n",
    "<p><b>Task 2.3: </b> In the pairwise density plots, try playing around with the option 'bw_method' by setting it to scalar values between [0.2, 1]. How does this affect the density plots? Do you still think there are only 2 states? Do this below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set layout for pairwise plot - 3 X 3 plot grid\n",
    "g = sns.PairGrid(behavior_df,  vars=['running_speed', 'pupil_area', 'lick_count'], diag_sharey=False)\n",
    "\n",
    "# Plot 2D density plot in the lower triangle \n",
    "width = 0.6 # Explore range between 0.2 and 1\n",
    "\n",
    "g.map_lower(sns.kdeplot, hue=None, bw_method = width)\n",
    "\n",
    "# Hide the upper triangle \n",
    "def hide_current_axis(*args, **kwds):\n",
    "    # function to hide upper triangle of the pairwise plots\n",
    "    plt.gca().set_visible(False)\n",
    "g.map_upper(hide_current_axis)\n",
    "\n",
    "# Plot 1D density plot\n",
    "g.map_diag(sns.kdeplot, hue=None, legend=False, bw_method = width)\n",
    "\n",
    "# Formatting \n",
    "g.fig.set_size_inches(7,7)\n",
    "g.fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p> This visualization broadly showed two states that mapped well to our previous state definitions. But playing around with the density plots may have raised some doubts that there were only two states. </p>\n",
    "    \n",
    "<p> Does every peak deserve a behavior state? How can we define state boundaries while incorporating all behavior variables? Are there other behavior states? </p>\n",
    "\n",
    "<p> Moreover, when determining behavioral states, we are dealing with data that changes over time, where the current state may depend on the current observation and also on the previous state. How do we incorporate time information?! </p>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !! Michael Buice Whiteboard Session !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h4>Multivariate definition of behavioral states</h4>\n",
    "\n",
    "<p>The density plots suggest there may be multiple underlying behavioral states. One approach to identifying these states is to use unsupervised clustering methods to group similar observations across multiple behavioral variables, revealing distinct patterns that may correspond to different states.</p>\n",
    "\n",
    "<p>K-means and Gaussian Mixture Models (GMMs) are two such clustering methods. While K-means assigns each data point to a single cluster with a deterministic assignment, GMMs allow data points to belong to multiple clusters with varying degrees of membership probabilities, making them more flexible than K-means. Unlike K-means, GMMs can model clusters with different shapes and orientations, providing a more realistic representation of the underlying structure.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h3> K-means </h3>\n",
    "    \n",
    "<p> First, lets sort out our notation: \n",
    "<br> <b> Constants</b> \n",
    "    <ul>\n",
    "        <li>$N$ - number of data points. </li>\n",
    "        <li>$K$ - number of clusters/components </li>\n",
    "    </ul>\n",
    "\n",
    "<b> Data</b> \n",
    "    <ul>\n",
    "        <li> $\\mathbf{X} = \\{x_{n}\\}_{n=1}^N$ where $x_n \\in \\mathbb{R}^{D}$ is the $n^{th}$ data point </li>\n",
    "    </ul>\n",
    "    \n",
    "<b> Latent Variables </b> \n",
    "    <ul>\n",
    "        <li>$\\mathbf{Z} = \\{z_{n}\\}_{n=1}^N$ wher $z_n \\in \\{1, ..., K\\}$ is the cluster assignment for the $n^{th}$ data point </li>\n",
    "    </ul>\n",
    "    \n",
    "<b> Parameters</b>   \n",
    "    <ul> \n",
    "        <li> $\\boldsymbol{\\Theta} = \\{\\mu_k\\}_{k=1}^K$ where $\\mu_k \\in \\mathbb{R}^D$ is the mean of the $k$-th cluster </li>\n",
    "    </ul>\n",
    "    \n",
    "<h4>K-Means</h4>\n",
    "    K-Means is an algorithm for estimating the latent variables, $\\mathbf{Z}$, and the parameters, $\\boldsymbol{\\Theta}$, given the data, $\\mathbf{X}$. The algorithm alternates between two steps,\n",
    "    <ol>\n",
    "        <li><b>Assign</b> each data point to the closest cluster:\n",
    "            \\begin{align}\n",
    "            z_{n} = \\underset{k \\in \\{1,..., K\\}}{\\text{arg min}} || x_{n} - \\mu_{k}||_{2}\n",
    "            \\end{align}\n",
    "        </li>\n",
    "        <li><b>Update</b> the parameters to the mean of the assigned data points\n",
    "            \\begin{align}\n",
    "            \\mu_{k} = \\frac{1}{N_{k}}\\sum_{n = 1}^{K} w_{n,k} x_{n}, \n",
    "            \\end{align}\n",
    "            where \n",
    "            \\begin{align}\n",
    "            w_{n,k} &= \\begin{cases} 1 & \\text{if } z_n =k \\\\ 0 & \\text{o/w} \\end{cases} \\\\\n",
    "            N_k &= \\sum_{n=1}^{N} w_{n,k}.\n",
    "            \\end{align}\n",
    "        </li>\n",
    "    </ol>\n",
    "    \n",
    "\n",
    "We can think of $w_{n,k}$ as the <i>weight</i> or <i>responsibility</i> that data point $n$ assigns to cluster $k$. \n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<h4>Gaussian Mixture Model (GMM)</h4>\n",
    "A GMM is a probabilistic model that specifies a joint distribution over data and latent variables given parameters, \n",
    "\\begin{align}\n",
    "    p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta}) \n",
    "    &= \\prod_{n=1}^N p(x_n \\mid z_n, \\boldsymbol{\\Theta}) \\, p(z_n \\mid \\boldsymbol{\\Theta}) \\\\\n",
    "    &= \\prod_{n=1}^N \\mathcal{N}(x_n \\mid \\mu_{z_n}, I) \\, \\mathrm{Cat}(z_n \\mid \\tfrac{1}{K} \\boldsymbol{1}_K)\n",
    "\\end{align}\n",
    "It is a <b>generative model</b> in that we can sample latent variables and data points from the model. In one dimension, it looks like this:\n",
    "<center><img src=\"../code/Resources/GMM.png\" width=\"400\" height=\"500\" > </center>\n",
    "<p>    \n",
    "The nice thing about thinking in terms of generative models is that it allows us to extend the model in various ways. For example, \n",
    "    <ol>\n",
    "        <li>We can let the clusters have <b>anisotropic covariance matrices</b> by changing the model to, \n",
    "        \\begin{align}\n",
    "            p(x_n \\mid z_n, \\boldsymbol{\\Theta})\n",
    "            &= \\mathcal{N}(x_n \\mid \\mu_{z_n}, \\Sigma_{z_n})\n",
    "        \\end{align}\n",
    "        where now the parameter set consists of both means and covariance matrices, $\\boldsymbol{\\Theta} = \\{\\mu_k, \\Sigma_k\\}_{k=1}^K$. \n",
    "        <br>\n",
    "        Then the assignment and update steps change to,\n",
    "        \\begin{align*}\n",
    "            w_{n,k} &= \\frac{\\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\mathcal{N}(x_n \\mid \\mu_j, \\Sigma_j)} \\\\\n",
    "            \\Sigma_j &= \\frac{1}{N_k} \\sum_{n=1}^N w_{n,k} (x_n - \\mu_k) (x_n - \\mu_k)^\\top\n",
    "        \\end{align*}\n",
    "        </li>\n",
    "        <li>We can allow for unbalanced classes by changing the model to,\n",
    "        \\begin{align}\n",
    "            p(z_n \\mid \\boldsymbol{\\Theta})\n",
    "            &= \\mathrm{Cat}(z_n \\mid \\boldsymbol{\\pi})\n",
    "        \\end{align}\n",
    "        where now the parameter set includes cluster probabilities, $\\boldsymbol{\\Theta} = \\{\\mu_k, \\Sigma_k, \\pi_k\\}_{k=1}^K$ such that $\\sum_k \\pi_k = 1$. \n",
    "        <br>\n",
    "        Then the assignment and update steps change to,\n",
    "        \\begin{align*}\n",
    "            w_{n,k} &= \\frac{\\pi_k \\mathcal{N}(x_n \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(x_n \\mid \\mu_j, \\Sigma_j)} \\\\\n",
    "            \\pi_k &= \\frac{N_k}{N}\n",
    "        \\end{align*}\n",
    "        </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h4>Expectation-Maximization (EM)</h4>\n",
    "Consider the following tweak to K-Means. Instead of setting the weight to 0 or 1, set it to,\n",
    "\\begin{align}\n",
    "    w_{n,k} &= \\frac{\\mathcal{N}(x_n \\mid \\mu_k, I)}{\\sum_{j=1}^K \\mathcal{N}(x_n \\mid \\mu_j, I)}\n",
    "\\end{align}\n",
    "where $\\mathcal{N}(x \\mid \\mu, \\Sigma)$ denotes the <b>probability density function (pdf)</b> of a point $x_n$ under a <b>multivariate normal distribution</b> with mean $\\mu$ and covariance $\\Sigma$.\n",
    "<br>\n",
    "Then the resulting algorithm corresponds to the expectation-maximization (EM) algorithm. EM is a generic algorithm of estimating the parameters of latent variable models. In this case, it estimates the parameters $\\boldsymbol{\\Theta}$ of a Gaussian mixture model.\n",
    "    \n",
    "<h4>What are K-Means and EM really doing?</h4>\n",
    "From this probabilistic perspective, we can now understand K-Means and EM as two closely related estimation algorithms. It turns out that K-Means is doing coordinate ascent on the joint probability, $p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta})$, alternating between assigning data points to clusters and updating the parameters. By contrast, EM is performing coordinate ascent on a <i>lower bound</i> on the marginal probability, $p(\\mathbf{X} \\mid \\boldsymbol{\\Theta}) = \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z} \\mid \\boldsymbol{\\Theta})$. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px;  padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h2> How should we incorporate time? </h2>\n",
    "<p> But its not simply enough to fit a Gaussian distribution to our behavioral data. While GMMs are a powerful tool for clustering and learning probabilistic associations in our data,they assume that each data point is independent of others. When determining behavioral states, we are dealing with data that changes over time, where the current state may depend on the current observation and also on the previous state. A choice which fulfills the goal are state-space models. \n",
    "    \n",
    "<p> State space models are a framework used to understand and predict complex systems that change over time. They are particularly useful when dealing with dynamic processes that might not be directly observable but can still influence the observed data. Sound familiar? \n",
    "    \n",
    "<p> The state-space model that we will be exploring in this workshop is the Gaussian hidden Markov model (HMM). \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<h4> Gaussian Hidden Markov Model (HMM) </h4>\n",
    "<p> A Gaussian HMM introduces the notion of dynamics to state determination. The behavior of an HMM is fully determined by three probabilties: \n",
    "  \n",
    "<ol>\n",
    "    <li> <b> Transition probability</b> $p(z_{t+1} \\mid z_{t})$, the probability of $z_{t+1}$ given its previous state $z_t$.  We can describe the transition probability by a K $\\times$ K matrix which is called transition matrix. The $(i,j)^{th}$ element of the matrix denotes the probability of the state transiting from the state $i$ to state $j$.\n",
    "    </li>\n",
    "    <li> <b> Emission probability </b> $p(x_{t}\\mid z_{t})$, the probability of the observation $x_{t}$ given its hidden state $z_{t}$. Specifically, for a Gaussian HMM, emission probabilties take the following form, \n",
    "    \\begin{align}\n",
    "    p(x_t \\mid z_t, \\boldsymbol{\\Theta}) &= \\mathcal{N}(x_t \\mid \\mu_{z_t}, \\Sigma_{z_t})\n",
    "    \\end{align}\n",
    "    where the emission parameters $\\boldsymbol{\\Theta} = \\{(\\mu_k, \\Sigma_k)\\}_{k=1}^K$ include the means and covariances for each of the $K$ discrete states.\n",
    "    </li>\n",
    "    <li> <b> Initial state distribution </b> $p(z_{1})$ \n",
    "    </li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"left\">Hidden Markov models</h4> \n",
    "\n",
    "<center><img src=\"../code/Resources/gHMM.png\" width=\"300\" height=\"400\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamax and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<p>So, lets try out HMM models! First, let's take a moment to import our minmal set of packages. This might take a few minutes. \n",
    "    \n",
    "<b>Dynamax</b> is a package created in collaboration between the Linderman lab and Google building around a <b>JAX engine</b>. <b>JAX</b> is a functional language which handles the backend for fast computation while presenting similarly to numpy. This means that you'll want to carry state along between calls. \n",
    "  \n",
    "<b>Dynamax</b> implements a variety of Gaussian HMMs with different constraints on the parameters (e.g. diagonal, spherical, and tied covariances). It also includes prior distributions on the parameters.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM-related imports from JAX, Dynamax, and TensorFlow Probability\n",
    "from functools import partial\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from dynamax.hidden_markov_model import GaussianHMM\n",
    "from dynamax.utils.utils import find_permutation\n",
    "from dynamax.utils.plotting import CMAP, COLORS, white_to_color_cmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000;  border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "From your prospective, working with <b>JAX</b> is going to work out to be about the same as working with numpy. In fact, many of the function calls we would use with numpy (e.g. <b>np.array</b>, <b>np.eye</b>, <b>np.tile</b>, etc.) have an equivalent <b>JAX</b> command that can be used in the same way (e.g. <b>jnp.array</b>, <b>jnp.eye</b>, <b>jnp.tile</b>, etc.). What happens under the hood might be different, but we don't need to worry about that today.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it out\n",
    "jnp.eye(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000;  border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Similarly, the Dynamax package maintains an interface very similar to the <b>sklearn</b> interface you saw this morning. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an HMM object for fitting\n",
    "true_num_states = 4\n",
    "observations_dim = 2\n",
    "hmm = GaussianHMM(true_num_states, observations_dim)\n",
    "hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have an HMM object with features we can look at.\n",
    "hmm.num_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm.emission_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Gaussian HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000;  border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Now that we have our software ready, lets try let's start with using Dynamax to introduce Gaussian HMMs on synthetic data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an HMM\n",
    "true_num_states = 4\n",
    "observations_dim = 2\n",
    "hmm = GaussianHMM(true_num_states, observations_dim)\n",
    "\n",
    "# 1. Set the initial state distribution\n",
    "initial_probs = jnp.ones(true_num_states) / true_num_states\n",
    "\n",
    "# 2. Make a transition matrix\n",
    "transition_matrix = 0.80 * jnp.eye(true_num_states) \\\n",
    "    + 0.15 * jnp.roll(jnp.eye(true_num_states), 1, axis=1) \\\n",
    "    + 0.05 / true_num_states\n",
    "\n",
    "# 3. Set the the emission probabilities - mean and covariances\n",
    "emission_means = jnp.column_stack([\n",
    "    jnp.cos(jnp.linspace(0, 2 * jnp.pi, true_num_states + 1))[:-1],\n",
    "    jnp.sin(jnp.linspace(0, 2 * jnp.pi, true_num_states + 1))[:-1],\n",
    "    jnp.zeros((true_num_states, observations_dim - 2)),\n",
    "    ])\n",
    "emission_covs = jnp.tile(0.25**2 * jnp.eye(observations_dim), (true_num_states, 1, 1))\n",
    "\n",
    "# Construct the HMM - this will be our ground truth \n",
    "true_params, _ = hmm.initialize(initial_probs=initial_probs,\n",
    "                                transition_matrix=transition_matrix,\n",
    "                                emission_means=emission_means,\n",
    "                                emission_covariances=emission_covs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: rgb(32, 177, 13); border-radius: 3px; padding: 10px; color: white;\"\">\n",
    "<p><b>Task 2.4: </b> Plot the transition matrix. How would you expect this model to behave?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Task 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate simulated data with our HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000;  border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Our model here is defined entirely by the parameters in the last cell. We can use this model to simulate data to get us started!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMMS are stochastic. \n",
    "#This means that, for reproducibility, we need to seed our random number generator \n",
    "# This line seeds the JAX random generator in a way that the HMM will be happy with!\n",
    "key = jr.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate 100 samples from our HMM\n",
    "num_timesteps = 100\n",
    "true_states, true_observations = hmm.sample(key=key, params=true_params, num_timesteps=num_timesteps)\n",
    "true_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_observations[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000;  border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Great! We have some data! Let try plotting it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for plotting\n",
    "def plot_gaussian_hmm(hmm, params, emissions, states,  title=\"Emission Distributions\", alpha=0.25):\n",
    "    \"\"\"\n",
    "    Visualize a 2D Gaussian HMM by plotting emission distributions and observed data trajectories.\n",
    "    \n",
    "    Creates a comprehensive plot showing:\n",
    "    - Contour plots of each state's Gaussian emission distribution\n",
    "    - Observed data points colored by their true hidden state\n",
    "    - Connected trajectory path showing temporal sequence\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hmm : GaussianHMM\n",
    "        A fitted Gaussian Hidden Markov Model from dynamax with 2D emissions.\n",
    "    params : dict or NamedTuple\n",
    "        HMM parameters containing emission means, covariances, and transition matrices.\n",
    "        Typically obtained from hmm.initialize() or hmm.fit().\n",
    "    emissions : array_like, shape (T, 2)\n",
    "        Sequence of 2D observations/emissions over time T.\n",
    "    states : array_like, shape (T,)\n",
    "        True hidden state sequence corresponding to each emission.\n",
    "        Values should be integers in range [0, hmm.num_states).\n",
    "    title : str, optional\n",
    "        Plot title. Default is \"Emission Distributions\".\n",
    "    alpha : float, optional\n",
    "        Transparency level for data points and trajectory line (0-1). Default is 0.25.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays the plot using matplotlib.\n",
    "    \"\"\"\n",
    "    lim = 1.1 * abs(emissions).max()\n",
    "    XX, YY = jnp.meshgrid(jnp.linspace(-lim, lim, 100), jnp.linspace(-lim, lim, 100))\n",
    "    grid = jnp.column_stack((XX.ravel(), YY.ravel()))\n",
    "\n",
    "    plt.figure()\n",
    "    for k in range(hmm.num_states):\n",
    "        lls = hmm.emission_distribution(params, k).log_prob(grid)\n",
    "        plt.contour(XX, YY, jnp.exp(lls).reshape(XX.shape), cmap=white_to_color_cmap(COLORS[k]))\n",
    "        plt.plot(emissions[states == k, 0], emissions[states == k, 1], \"o\", mfc=COLORS[k], mec=\"none\", ms=5, alpha=alpha)\n",
    "\n",
    "    plt.plot(emissions[:, 0], emissions[:, 1], \"-k\", lw=1, alpha=alpha)\n",
    "    plt.xlabel(\"$y_1$\")\n",
    "    plt.ylabel(\"$y_2$\")\n",
    "    plt.title(title)\n",
    "    plt.gca().set_aspect(1.0)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize emission distributions, observation sequence, and true_states \n",
    "plot_gaussian_hmm(hmm, true_params, true_observations, true_states, \n",
    "                  title=\"Observations and emission distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_hmm_data(hmm, params, emissions, states, xlim=None, title = \"Simulated data from an HMM\"):\n",
    "    \"\"\"\n",
    "    Create a time-series visualization of HMM emissions with hidden state background.\n",
    "    \n",
    "    Generates a multi-panel plot showing how observed emissions evolve over time,\n",
    "    with background colors indicating the active hidden state at each time step.\n",
    "    Each emission dimension gets its own subplot with observed data, expected means,\n",
    "    and color-coded state information.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hmm : GaussianHMM\n",
    "        A Gaussian Hidden Markov Model from dynamax.\n",
    "    params : dict or NamedTuple\n",
    "        HMM parameters containing emission means and other model parameters.\n",
    "        Must include params.emissions.means with shape (num_states, emission_dim).\n",
    "    emissions : array_like, shape (T, D)\n",
    "        Sequence of D-dimensional observations over T time steps.\n",
    "    states : array_like, shape (T,)\n",
    "        Hidden state sequence corresponding to each time step.\n",
    "        Values should be integers in range [0, hmm.num_states).\n",
    "    xlim : tuple of (float, float), optional\n",
    "        Time axis limits as (start, end). If None, uses full data range (0, T).\n",
    "    title : str, optional\n",
    "        Main plot title. Default is \"Simulated data from an HMM\".\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        The figure object containing the plot.\n",
    "    axs : array of matplotlib.axes.Axes\n",
    "        Array of subplot axes, one per emission dimension.\n",
    "    \"\"\"\n",
    "    num_timesteps = len(emissions)\n",
    "    emission_dim = hmm.emission_dim\n",
    "    means = params.emissions.means[states]\n",
    "    lim = 1.05 * abs(emissions).max()\n",
    "\n",
    "    # Plot the data superimposed on the generating state sequence\n",
    "    fig, axs = plt.subplots(emission_dim, 1, sharex=True, figsize = (15, 5))\n",
    "    \n",
    "    for d in range(emission_dim):    \n",
    "        axs[d].imshow(states[None, :], aspect=\"auto\", interpolation=\"none\", cmap=CMAP,\n",
    "                      vmin=0, vmax=len(COLORS) - 1, extent=(0, num_timesteps, -lim, lim))\n",
    "        axs[d].plot(emissions[:, d], \"-k\")\n",
    "        axs[d].plot(means[:, d], \":k\")\n",
    "        axs[d].set_ylabel(\"$y_{{t,{} }}$\".format(d+1))\n",
    "        \n",
    "    if xlim is None:\n",
    "        plt.xlim(0, num_timesteps)\n",
    "    else:\n",
    "        plt.xlim(xlim)\n",
    "\n",
    "    axs[-1].set_xlabel(\"Time steps\")\n",
    "    axs[0].set_title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gaussian_hmm_data(hmm, true_params, true_observations, true_states);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: rgb(32, 177, 13); border-radius: 3px; padding: 10px; color: white;\"\">\n",
    "<p><b>Task 2.5: </b> How does the HMM data change as you alter the transition matrix? Also, try changing the number of hidden states. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Starter code for you to change these parameters for task 2.5\n",
    "\"\"\"\n",
    "\n",
    "# Make an HMM and sample data and true underlying states\n",
    "true_num_states_2 = 4  # CHANGE THIS!!!!!!!!!!!\n",
    "observations_dim_2 = 2\n",
    "hmm_2 = GaussianHMM(true_num_states_2, observations_dim_2)\n",
    "\n",
    "# Use Jax to make a transition matrix.\n",
    "# CHANGE THESE TOO!!!!!!! \n",
    "transition_matrix_2 = 0.8 * jnp.eye(true_num_states_2) \\\n",
    "    + 0.15 * jnp.roll(jnp.eye(true_num_states_2), 1, axis=1) \\\n",
    "    + 0.05 / true_num_states_2                                   \n",
    "\n",
    "# Use Jax to set the the emission probabilities - mean and covariances\n",
    "emission_means_2 = jnp.column_stack([\n",
    "    jnp.cos(jnp.linspace(0, 2 * jnp.pi, true_num_states_2 + 1))[:-1],\n",
    "    jnp.sin(jnp.linspace(0, 2 * jnp.pi, true_num_states_2 + 1))[:-1],\n",
    "    jnp.zeros((true_num_states_2, observations_dim_2 - 2)),\n",
    "    ])\n",
    "emission_covs_2 = jnp.tile(0.25**2 * jnp.eye(observations_dim), (true_num_states, 1, 1))\n",
    "\n",
    "# Set the initial state distribution - change this to unequal probabilities.\n",
    "initial_probs_2 = jnp.ones(true_num_states_2) / true_num_states_2\n",
    "\n",
    "# Construct the HMM - this will be our ground truth \n",
    "true_params_2, _ = hmm_2.initialize(initial_probs=initial_probs_2,\n",
    "                                transition_matrix=transition_matrix_2,\n",
    "                                emission_means=emission_means_2,\n",
    "                                emission_covariances=emission_covs_2)\n",
    "\n",
    "# Sample your altered HMM\n",
    "num_timesteps_2 = 100\n",
    "key_2 = jr.split(jr.PRNGKey(0), 1)[0]\n",
    "true_states_2, observations_2 = hmm_2.sample(key = key_2, params=true_params_2, num_timesteps=num_timesteps_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot emissions and true_states for Task 2.4\n",
    "plot_gaussian_hmm(hmm_2, true_params_2, observations_2, true_states_2, \n",
    "                  title=\"Observation distributions\")\n",
    "\n",
    "plot_gaussian_hmm_data(hmm_2, true_params_2, observations_2, true_states_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use simulated data to estimate parameters of HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "So far, we have seen what the different parameters of our HMM do. \n",
    "\n",
    "But most of the time, we don't actually have the luxury of knowing our model parameters when we start--we need to recover them from our data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random key\n",
    "key = jr.PRNGKey(0)\n",
    "\n",
    "# Instatiate a model.\n",
    "# Lets start by assuming we know the number of states\n",
    "true_num_states = 4\n",
    "observations_dim = 2\n",
    "model = GaussianHMM(true_num_states, observations_dim) \n",
    "\n",
    "# Initialize the model using the observations from our \"ground truth\" simulated data\n",
    "params, props = model.initialize(key=key, method='kmeans', emissions=true_observations)\n",
    "\n",
    "fit_params, train_lps = model.fit_em(params, props, true_observations, \n",
    "                                             num_iters=100, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: rgb(32, 177, 13); border-radius: 3px; padding: 10px; color: white;\"\">\n",
    "<p><b>Task 2.6:</b>\n",
    "Visualize the transition matrix for the model we just fit. \n",
    "    \n",
    "Hint: Use <i>fit_params.transitions.transition_matrix</i> \n",
    "\n",
    "Does it look like you would expect?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for Task 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p> Reconstructing the transition matrix for a known model is, perhaps, to easy a problem. In most cases we won't have anywhere near this much prior information about the HMM we are trying to reconstruct. What is, for example, the true number of states in the mouse?\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> In previous days, we've seen the vitial importance of cross-validation as a sanity check on your data to avoid overfitting. Let's bring those principles in here.\n",
    "\n",
    "<p> To make this work, let's go ahead and simulate three \"chunks\" of data from our ground truth model. In \"real life,\" you will do this by splitting your data up. Here, we have the artificial luxury of simulating seperate chunks. \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating data into batches\n",
    "num_train_batches = 3\n",
    "num_timesteps= 100\n",
    "\n",
    "# Simulate 3 sets of data from the mode\n",
    "train_key = jr.split(jr.PRNGKey(0), 3)\n",
    "train_true_states = np.zeros((num_train_batches, num_timesteps)).astype(int)\n",
    "train_observations = np.zeros((num_train_batches, num_timesteps,  observations_dim))\n",
    "for k in range(num_train_batches):\n",
    "    train_true_states[k], train_observations[k] = hmm.sample(key = train_key[k], params=true_params, num_timesteps=num_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<p> Here are some functions that \"splits\" above simulated data to fit evaluate models.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_fold(model, params, props, y_train, y_val, num_iters=100):\n",
    "    \"\"\"\n",
    "    Train a model on training data and evaluate on validation data for one cross-validation fold.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : HMM or similar probabilistic model\n",
    "        The model object with fit_em() and marginal_log_prob() methods\n",
    "    params : dict or similar\n",
    "        Initial model parameters (means, covariances, transition probabilities, etc.)\n",
    "    props : dict or similar  \n",
    "        Additional model properties or metadata from initialization\n",
    "    y_train : array-like, shape (num_sequences, num_timesteps, obs_dim)\n",
    "        Training observations for this fold. Multiple sequences concatenated along\n",
    "        the first axis.\n",
    "    y_val : array-like, shape (num_timesteps, obs_dim)\n",
    "        Validation observations for this fold. Single sequence to evaluate on.\n",
    "    num_iters : int, default=100\n",
    "        Maximum number of EM iterations for training\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Marginal log-likelihood of the validation data given the trained model.\n",
    "        Higher values (less negative) indicate better model fit and generalization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit on the training batches, evaluate on the held-out batch\n",
    "    fit_params, train_lps = model.fit_em(params, props, y_train,\n",
    "                                         inputs=None, num_iters=num_iters, verbose=False)\n",
    "    return model.marginal_log_prob(fit_params, y_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model, key, train_observations, num_iters=100,):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation on a HMM or other probabilistic sequence model.\n",
    "    \n",
    "    This function performs k-fold cross-validation where k equals the number of\n",
    "    batches in train_observations.\n",
    "\n",
    "    Note that the function overrides fits on the passed model object on each fold.\n",
    "    Use \"copy/deepcopy\" to protect a model you do not wish to override.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : HMM or similar probabilistic model\n",
    "        The model object to be cross-validated. Must have initialize(), fit_em(),\n",
    "        and marginal_log_prob() methods. A deep copy is created for each fold.\n",
    "    key : jax.random.PRNGKey\n",
    "        Random key for reproducible initialization. Different keys are derived\n",
    "        for each fold using jax.random.fold_in().\n",
    "    train_observations : array-like, shape (num_batches, num_timesteps, obs_dim)\n",
    "        Batched observation sequences. Each batch along axis 0 becomes a validation\n",
    "        fold, with remaining batches used for training that fold.\n",
    "    num_iters : int, default=100\n",
    "        Maximum number of EM iterations for training each fold.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mean_val_ll : float\n",
    "        Mean validation log-likelihood across all folds. This is the primary\n",
    "        metric for model performance - higher values indicate better generalization.\n",
    "    val_lls : ndarray, shape (num_batches,)\n",
    "        Individual validation log-likelihood for each fold. Useful for assessing\n",
    "        consistency of performance across folds.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_train_batches = int(train_observations.shape[0])\n",
    "\n",
    "    val_lls = np.zeros(num_train_batches, dtype=float)\n",
    "\n",
    "    for k in range(num_train_batches):\n",
    "        # held-out validation batch\n",
    "        y_val = train_observations[k]\n",
    "        # all other batches concatenated along batch axis\n",
    "        y_train = jnp.concatenate([train_observations[:k], train_observations[k+1:]], axis=0)\n",
    "\n",
    "        # Each fold gets a radom key\n",
    "        k_key = jr.fold_in(key, k)\n",
    "        # Make the model for this fold\n",
    "        params, props = model.initialize(key=k_key, method='kmeans', emissions=y_train)\n",
    "        \n",
    "        val_lls[k] = float(fit_model_fold(model, params, props, y_train, y_val, num_iters))\n",
    "        \n",
    "    return float(val_lls.mean()), val_lls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<p> Now lets cross validate!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example for \n",
    "this_num_states = 4\n",
    "observation_dims = 2\n",
    "cv_model = GaussianHMM(this_num_states, observation_dims, transition_matrix_stickiness=10.) \n",
    "cv_key = jr.PRNGKey(0)\n",
    "mean_ll,fold_lls = cross_validate_model(cv_model,cv_key,train_observations)\n",
    "print(mean_ll)\n",
    "print(fold_lls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "<p> On its own, this log likelihood data isn't super useful. Log likelihood, for example, scales with the number of samples, so its tricky to compare accross datasets.\n",
    "\n",
    "<p> Where it can be useful, however, is in comparing models that are fit on the same data. \n",
    "\n",
    "<p> Lets go ahead and try this. We will fit models with different number of states, then use the log likelihoods to evalue which model is \"best.\"</p>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit models with different numbers of states to identify \"best\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a range of Gaussian HMMs\n",
    "all_num_states = list(range(2, 10))\n",
    "\n",
    "avg_val_lls = np.zeros(len(all_num_states))\n",
    "all_val_lls = np.zeros((len(all_num_states),3))\n",
    "\n",
    "for ii,this_num_states  in enumerate(all_num_states):\n",
    "    print(f\"fitting model with {this_num_states} states\")\n",
    "    this_model = GaussianHMM(this_num_states, observation_dims, transition_matrix_stickiness=10.) \n",
    "    mean_ll,fold_lls = cross_validate_model(this_model,cv_key,train_observations)\n",
    "    avg_val_lls[ii] = mean_ll\n",
    "    all_val_lls[ii,:] = fold_lls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Lets plot up the average validation log-likelihood and discuss with your neighbor the implications of this curve shape? If you didn't know how many true states there where, how would you decide when to stop adding states? Will this lesson generalize to other types of data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4, 3))\n",
    "for cv in range(num_train_batches):\n",
    "    plt.plot(all_num_states, np.array(all_val_lls)[:, cv], '.-', label = f'fold {cv}')\n",
    "plt.legend()\n",
    "plt.xlabel(\"num states ($K$)\")\n",
    "plt.ylabel(\"avg. validation log likelihood\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> How should we choose the right number of states? \n",
    "    \n",
    "<p> There's no single \"right\" answer for how to choose the number of states, but reasonable heuristics include:\n",
    "    \n",
    "<ul> * picking $K$ that has the highest average validation log likelihood\n",
    "<p> * picking $K$ where the average validation log likelihood stops increasing by a minimum amount\n",
    "<p> * picking $K$ with a hypothesis test for increasing mean\n",
    "<p> * picking $K$ consistent with the predicability-computability-stability (PCS) framework.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> Here, we'll just choose the number of states with the highest average validation log likelihood to determine the best number of states for our HMM.</p>\n",
    "<p> Note that this is \"OK\" because our folds are all the same size, so this falls into the special case where log likelihoods are safe to compare!</p>\n",
    "<p> Did we do a good job of recapitulating our ground truth? </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4, 3))\n",
    "std_dev= np.nanstd(all_val_lls, axis= 1)\n",
    "plt.errorbar(all_num_states, avg_val_lls, yerr = std_dev , color='k')\n",
    "plt.xlabel(\"num states ($K$)\")\n",
    "plt.ylabel(\"Avg. validation log likelihood\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_num_states = all_num_states[jnp.argmax(jnp.stack(avg_val_lls))]\n",
    "print(\"best number of states:\", best_num_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Now that we have a guess as to the \"best\" number of states, we can find the most likely state any any given instance in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "key = jr.PRNGKey(0)\n",
    "test_hmm = GaussianHMM(best_num_states, observations_dim, transition_matrix_stickiness=10.)\n",
    "params, props = test_hmm.initialize(key=key, method=\"kmeans\", emissions=true_observations)\n",
    "params, lps = test_hmm.fit_em(params, props, train_observations, num_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict states\n",
    "most_likely_states = test_hmm.most_likely_states(params, true_observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot true states\n",
    "plot_gaussian_hmm_data(hmm, true_params, true_observations, true_states, title = 'True states')\n",
    "\n",
    "# Plot inferred states\n",
    "plot_gaussian_hmm_data(test_hmm, params, true_observations, most_likely_states, title = 'Estimated states')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State determination in Visual Behavior Neuropixels data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "<p> We now kind of get a hang of HMM. So, let's take what we've done above and apply it to the behavior variables from the previous workshop to automatically detect states in our mice! Now, we have the capability to incorporate a multi-dimensional dataset to inform our state boundaries. \n",
    "<p> To accomplish this goal, we will fit our data to a Gaussian HMM. \n",
    "    \n",
    "<p> To complement our states from the previous workshop, lets extract the behavior states using only behavior data, that is, pupil size, running speed and lick counts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "Our first step will be to cast our data as <b>JAX</b> arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this model, it's important that the data is converted into a *JAX* array\n",
    "observations = jnp.array(behavior_df.values)\n",
    "num_trials, num_dimensions = observations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "You will notice that all of these data are in very different values. Remember the <b>StandardScaler</b> from this AM? It can help us deal with these very differently scaled data by Z-scoring them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First scale the dimensions of the data to be normalized\n",
    "scaler = StandardScaler()    \n",
    "observations = scaler.fit_transform(observations)\n",
    "observations[:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "Finally, lets parameterize our cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, lets parameterize our cross validiation\n",
    "n_batches = 6\n",
    "n_steps = num_trials - (num_trials % n_batches)\n",
    "batched_observations = observations[:n_steps, :].reshape(n_batches, -1, observations.shape[1])\n",
    "batch_size = batched_observations.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "One very quick mathematical aside: <b>Dynamax</b> can't handle num_states = 1 (a simple way of thinking about this is that a one state model isn't really a mixture of states, is it?). But what if our mouse is in the same state the whole time?\n",
    "\n",
    "The good news is that is is pretty straightforward to use a maximum likelihood estimator to compute the log likelihood for a single gaussian fit to the data. Here is a function that uses <b>JAX</b> to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import multivariate_normal as mvn\n",
    "\n",
    "def _gaussian_mle_loglik(y_train, y_val, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood of validation data under a multivariate Gaussian\n",
    "    fitted via maximum likelihood estimation on training data.\n",
    "    \n",
    "    Fits a multivariate Gaussian distribution to the training data using MLE\n",
    "    (sample mean and unbiased sample covariance), then evaluates the total\n",
    "    log-likelihood of the validation data under this fitted distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_train : array_like, shape (B_train, T, D) or (N, D)\n",
    "        Training data. If 3D, represents B_train batches of T timesteps with\n",
    "        D-dimensional observations. If 2D, represents N observations with\n",
    "        D dimensions. Will be flattened to (N_train, D) for MLE fitting.\n",
    "    y_val : array_like, shape (T, D) or (B_val, T, D)\n",
    "        Validation data to evaluate log-likelihood on. If 3D, will be flattened\n",
    "        to (N_val, D). If 2D, used as-is.\n",
    "    eps : float, default=1e-6\n",
    "        Regularization parameter added to diagonal of covariance matrix for\n",
    "        numerical stability (prevents singular matrices).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Total log-likelihood of all validation samples under the fitted\n",
    "        multivariate Gaussian distribution.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Flatten batch/time for MLE\n",
    "    Yt = jnp.reshape(y_train, (-1, y_train.shape[-1]))  # (N_train, D)\n",
    "    mu = jnp.mean(Yt, axis=0)                           # (D,)\n",
    "    # Unbiased sample cov; regularize for numeric stability\n",
    "    Yc = Yt - mu\n",
    "    # (D,D)\n",
    "    Sigma = (Yc.T @ Yc) / jnp.maximum(Yt.shape[0] - 1, 1)\n",
    "    Sigma = Sigma + eps * jnp.eye(Sigma.shape[0], dtype=Sigma.dtype)\n",
    "\n",
    "    # Evaluate log-likelihood on validation\n",
    "    if y_val.ndim == 3:  # (B_val, T, D)\n",
    "        Yv = jnp.reshape(y_val, (-1, y_val.shape[-1]))\n",
    "    else:                # (T, D)\n",
    "        Yv = y_val\n",
    "    ll = jnp.sum(mvn.logpdf(Yv, mu, Sigma))\n",
    "    return float(ll)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "We now need to update our cross validation code to handle this new case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model, key, train_observations, num_iters=100):\n",
    "    \"\"\"\n",
    "    This function is the same as the one above, \n",
    "    but with functionality to handle 1-state models\n",
    "    \"\"\"\n",
    "    num_train_batches = int(train_observations.shape[0])\n",
    "    val_lls = np.zeros(num_train_batches, dtype=float)\n",
    "\n",
    "    for k in range(num_train_batches):\n",
    "        y_val = train_observations[k]\n",
    "        y_train = jnp.concatenate([train_observations[:k], train_observations[k+1:]], axis=0)\n",
    "\n",
    "        if model.num_states == 1:\n",
    "            # Bypass Dynamax EM/prior for 1-state; use Gaussian MLE baseline.\n",
    "            val_lls[k] = _gaussian_mle_loglik(y_train, y_val)\n",
    "            continue\n",
    "\n",
    "        # K >= 2: normal Dynamax path\n",
    "        k_key = jr.fold_in(key, k)\n",
    "        params, props = model.initialize(key=k_key, method='kmeans', emissions=y_train)\n",
    "        val_lls[k] = float(fit_model_fold(model, params, props, y_train, y_val, num_iters))\n",
    "\n",
    "    return float(val_lls.mean()), val_lls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "Great! We are now ready for the <b>\"Main Event\"</b> some model fitting with <b>real</b> data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random key so things dont change each time we run the cell\n",
    "key = jr.PRNGKey(0)\n",
    "\n",
    "# Number of states to test\n",
    "num_states_range = np.arange(1, 9)\n",
    "\n",
    "# Define empty lists so that we have place to put our data.\n",
    "avg_test_lls = np.empty(len(num_states_range))\n",
    "all_test_lls = np.empty([len(num_states_range),n_batches])\n",
    "std_test_lls = np.empty(len(num_states_range))\n",
    "\n",
    "for ii,num_states in enumerate(num_states_range):\n",
    "    hmm = GaussianHMM(num_states, batched_observations.shape[-1], transition_matrix_stickiness=5)\n",
    "    \n",
    "    print(f\"\\n{'='*40}\\nTraining model with {num_states} state(s)\\n{'='*40}\")\n",
    "    \n",
    "    avg_ll, lls = cross_validate_model(\n",
    "        hmm, key, batched_observations, num_iters=100\n",
    "    )\n",
    "    avg_test_lls[ii] = avg_ll\n",
    "    all_test_lls[ii,:] = lls.flatten()\n",
    "    std_test_lls[ii] = np.std(all_test_lls[ii,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Lets plot the results of our cross validation to see number of states yeilds the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4, 3))\n",
    "plt.errorbar(num_states_range, avg_test_lls, yerr = np.array(std_test_lls)/np.sqrt(n_batches), \n",
    "             mfc = 'w', color = 'k', marker = 'o')\n",
    "plt.legend(['Mean + S.E.M'])\n",
    "plt.xlabel(\"number of states\")\n",
    "plt.ylabel(\"average test log prob\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(0)\n",
    "number_of_states = 3\n",
    "final_hmm = GaussianHMM(number_of_states, num_dimensions, transition_matrix_stickiness=5)\n",
    "params, param_props = final_hmm.initialize(key=key, method=\"kmeans\", emissions=jnp.array(observations))\n",
    "params, lps = final_hmm.fit_em(params, param_props, jnp.array(observations), num_iters=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<p> Lets plot the inferred states from the \"best\" model we just fit </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most likely discrete states given the learned model parameters\n",
    "most_likely_states = final_hmm.most_likely_states(params, observations)\n",
    "fig, axs = plot_gaussian_hmm_data(final_hmm, params, observations, most_likely_states, title = f'{session_id}')\n",
    "\n",
    "# Formatting \n",
    "xticks = axs[0].get_xticks().astype(int)[:-1]\n",
    "for n, ax in enumerate(axs):\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(go_trials.index.values[xticks])\n",
    "    ax.set_ylabel(f'{behavior_df.keys()[n]}')\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "axs[-1].set_xlabel('Go trials')\n",
    "\n",
    "fig.set_size_inches(15,7)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "Finally, lets bring this full circle. Lets plot the result of our model relative to our the thresholding method we used at the beginning of this tutorial. Did we uncover anything new?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot state boundaries for HMM fit\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 6), sharex=True)\n",
    "hmm_states = most_likely_states\n",
    "ax[0].imshow(hmm_states[None, :], aspect=\"auto\", interpolation=\"none\", cmap=CMAP, vmin=0, vmax=len(COLORS) - 1,  extent=[0, len(hmm_states), -0.1, 1.1]  )\n",
    "ax[0].plot(hit_rate.values, color = 'k', lw = 2)\n",
    "\n",
    "# Plot states from manual threshold\n",
    "ax[1].imshow(states.values[None, :], aspect=\"auto\", interpolation=\"none\", cmap=CMAP, vmin=0, vmax=len(COLORS) - 1,  extent=[0, len(states), -0.1, 1.1]  )\n",
    "# for state_id in range(np.nanmax(states)+1): \n",
    "#     ax[1].fill_between(go_trials.index, 0, 1, where = states == state_id, alpha=0.5, color = cmap[state_id], ls = 'None', label = f'state {state_id}')\n",
    "ax[1].plot(hit_rate.values, color = 'k', lw = 2)\n",
    "\n",
    "# Formatting \n",
    "ax[0].set_title('HMM states')\n",
    "ax[1].set_title('Manual thresholding')\n",
    "ax[1].set_xlabel(\"Go trials\")\n",
    "xticks = ax[0].get_xticks().astype(int)[:-1]\n",
    "for j in range(2): \n",
    "    ax[j].set_ylabel('Hit rate')\n",
    "    ax[j].set_xticks(xticks)\n",
    "    ax[j].set_xticklabels(go_trials.index.values[xticks])\n",
    "    handles, labels = ax[j].get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax[j].legend(by_label.values(), by_label.keys(), bbox_to_anchor = (1.05, 1), fontsize = 10)\n",
    "    ax[j].spines[\"top\"].set_visible(False)\n",
    "    ax[j].spines[\"right\"].set_visible(False)\n",
    "plt.suptitle(f'Session id: {session_id}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; border-radius: 3px; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "This brings us to the end of this session. In the homework you will get a chance to dive into these state definitions more deeply and to ask whether neural population decoding varies between states."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
