{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../code/Resources/cropped-SummerWorkshop_Header.png\"> \n",
    "\n",
    "<h1 align=\"center\">Workshop 1: Tutorial on neuronal decoding and behavior</h1> \n",
    "<h3 align=\"center\">Summer Workshop on the Dynamic Brain</h3> \n",
    "<h3 align=\"center\">Thursday, August 26th, 2025</h3> \n",
    "<h4 align=\"center\">Day 2</h4> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "# 0.0 Neural Coding \n",
    "\n",
    "Neural coding describes how neurons represent information about the world. Coding can be studied by asking whether external or internal events lead to changes in neural activity (<b>encoding</b>), or by asking whether different types of information can be read out from neural activity (<b>decoding</b>). In this workshop we will focus on this later problem. Specifically, we will try to read out information about stimulus identity from neurons recorded during the Dynamic Routing Task. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "### 0.1 The 'visual change detection' task\n",
    "\n",
    "For an example today to start to understand encoding and decoding, we are going to look at the dataset including the visual change detection task. Allen folks like to refer to this as the \"Visual Behavior Task\" dataset.\n",
    "\n",
    "There is a very nice description of this task in the <b> data book </b>. Lets start by reminding ourselves how this task works:\n",
    "\n",
    "https://allenswdb.github.io/physiology/stimuli/visual-behavior/VB-Behavior.html#change-detection-task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "### 0.2 Our questions \n",
    "\n",
    "- How can we decode information from neurons and populations of neurons\n",
    "- How do we decide if our decoding is any good?\n",
    "- Can we use these tools to learn something about neural dyanimcs in visual cortex?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "# 1.0 Setup\n",
    "\n",
    "To Python, we first need packages Lets start with some you are familiar with. <b>Numpy</b>, <b>pandas</b>, and <b>matplotlib</b> should be favorites of yours by now. In addition, lets grab your new friend <b>pynwb</b> so that we can actually look some data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start by importing some basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# pynwb is for reading python NWB files\n",
    "import pynwb\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "We also need to point our computer to the data. This will depend a bit on how you choose to do your compute. For CodeOcean, the platform is <i> 'amzn'</i>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from pathlib import Path\n",
    "platstring = platform.platform()\n",
    "\n",
    "if 'Darwin' in platstring:\n",
    "    # macOS \n",
    "    data_root = Path(\"/Volumes/Brain2023/\")\n",
    "elif 'Windows'  in platstring:\n",
    "    # Windows (replace with the drive letter of USB drive)\n",
    "    data_root = Path(\"E:/\")\n",
    "elif ('amzn' in platstring):\n",
    "    # then on CodeOcean\n",
    "    data_root = Path(\"/data/\")\n",
    "else:\n",
    "    # then your own linux platform\n",
    "    # EDIT location where you mounted hard drive\n",
    "    data_root = Path(\"/media/$USERNAME/Brain2025/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 1.1 An example session.\n",
    "\n",
    "We will all use the same one to get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_session = 1139846596 # Other good ones to play with: 1152811536, 1069461581\n",
    "this_session = str(example_session)\n",
    "this_filename = f'ecephys_session_{this_session}.nwb'\n",
    "nwb_path = data_root/'visual-behavior-neuropixels'/'behavior_ecephys_sessions'/this_session/this_filename\n",
    "print(nwb_path)\n",
    "# And read the nwb\n",
    "session = pynwb.NWBHDF5IO(nwb_path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at the session object.\n",
    "session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 1.2 The trials table\n",
    "\n",
    "Remember that the trial table lists all the per-trial event data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trials data\n",
    "trials = session.trials.to_dataframe()\n",
    "trials.iloc[20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 1.3 Stimulus table(s)\n",
    "\n",
    "But wait...We were trying to decode images.\n",
    "\n",
    "The 'trials' table for this task is designed around the change task structure. However, there are other ways to look at these data- for example, by image presentations, by flash presentations, etc. \n",
    "\n",
    "These data are stored in the 'intervals' section of the nwb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli = session.intervals['Natural_Images_Lum_Matched_set_ophys_H_2019_presentations'].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "There are both active and passive stimuli in this table. For now, lets only look at active trials. Trying to distinguish between these cases might be a good place to start a project, later! \n",
    "\n",
    "Read more about the task strucutre here: https://allenswdb.github.io/physiology/ephys/visual-behavior/VB-Neuropixels.html#experiment-design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_stimuli = stimuli[stimuli.active == True]\n",
    "# passive_stimuli = stimuli[stimuli.active == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "#### Visualize the stimulus table\n",
    "\n",
    "Lets plot the information in the stimulus table to get a better sense of how its organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a moment to highlight my favorite numpy command!\n",
    "# Get the stimulus shown for each spike trial\n",
    "unq_stim,stim_id, = np.unique(active_stimuli.image_name,return_inverse = True)\n",
    "print(unq_stim)\n",
    "print(stim_id[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new plot. We need a tall one for this.\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# This is just some fancy code to make the discrete colors work\n",
    "cmap = mpl.cm.get_cmap('tab10', \n",
    "                       len(unq_stim))\n",
    "\n",
    "norm = mpl.colors.BoundaryNorm(boundaries=np.arange(len(unq_stim)+1)-0.5,\n",
    "                               ncolors=len(unq_stim))\n",
    "\n",
    "# Count the index of each flash in each trial\n",
    "_,trl_idx,trl_counts = np.unique(active_stimuli.trials_id,return_inverse=True,return_counts = True)\n",
    "event_indices = np.zeros_like(trl_idx)\n",
    "for i in range(len(trl_counts)):\n",
    "    event_indices[trl_idx == i] = np.arange(trl_counts[i])\n",
    "\n",
    "# The actual plotting code\n",
    "z = ax.scatter(event_indices,\n",
    "               trl_idx,\n",
    "               s = 50,\n",
    "               c = stim_id.T,\n",
    "               cmap = cmap,\n",
    "               norm = norm\n",
    "              )\n",
    "\n",
    "# Label stuff!\n",
    "ax.set_xlabel('Flash in trial')\n",
    "ax.set_ylabel('Trial ID')\n",
    "ax.set_ylim([0,stimuli.trials_id.values[-1]])\n",
    "ax.set_title('Task image stimuli')\n",
    "\n",
    "# Get the color key\n",
    "tick_locs = np.arange(len(unq_stim))\n",
    "cbar = plt.colorbar(z, ticks=tick_locs)\n",
    "cbar.set_label('Image identity')\n",
    "cbar.set_ticklabels(unq_stim)\n",
    "\n",
    "ax.set_ylim([0,20])\n",
    "ax.set_yticks(np.arange(0,21));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 1.4 The units table\n",
    "\n",
    "Now that we have our stimuli, we need unit information! Just as the trials table contains information about each trial, the \"units\" table contains a good chunk of information about each unit!. Lets take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get units table \n",
    "units_table = session.units.to_dataframe()\n",
    "units_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "    \n",
    "<h3> How many units are in the units table?? </h3>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 1.5 Where are our units??\n",
    "\n",
    "The units table doesn't actually tell us where are units were recorded from (it does in some dataset, just not this one). Instead, it stores the electrode id of the peak channel that that unit was recorded from. Where were these channels? for that we need to look at the electrodes table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electrodes_table = session.electrodes.to_dataframe()\n",
    "electrodes_table.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Data in the electrodes table are organized by unique channel identifiers. This setup allows us to do a dataframe join on the peak_channel_id from the units table. The result of this operation will be a table that contains all of the unit information, PLUS the electrode information for the key-ed channel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_electrode_table = units_table.join(electrodes_table,on = 'peak_channel_id')\n",
    "print('Lenght before join: ' + str(len(units_table)))\n",
    "print('Lenght after join: ' + str(len(units_electrode_table)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "    \n",
    "<h3> Take a look at the units table. </h3>\n",
    "\n",
    "Can you find which brain area each unit is localized to now? hint: use .head or .iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 1.6 Unit QC\n",
    "\n",
    "Thats a lot of units. But do we trust them all?\n",
    "\n",
    "The truth is, one of the biggest challenges with ephy analysis sorting units and figuring out which are safe to use. Neuropixels, by virtue of having many contacts near eachother, make it possible to record many units simultaniously. This is great, but it creates an additional challenge of knowing which units are \"good\" for analysis. Because large recordings are almost always sorted algorithmically, selecting good units is particularly important because no human has manually identified which units are safe for further analysis.\n",
    "\n",
    "The units table contains many unit QC metrics. Typically, rather than just blindly trusting every output of the automated sorting, we typically impose some constraints on the data. Here are a few examples of useful ones:\n",
    "\n",
    "+ isi_violations_ratio: what fraction of spikes happen closer together than should be possible for a neuron. If this is too high, its a sign of eather non-neuronal noise or more than one neuron getting merged into a unit.\n",
    "+ amplitude_cutoff: An estimate of the fraction of spikes missed by the sorter, based on the amplitude historgram for the neuron.\n",
    "+ presence_ratio: recordings are not 100% static. If the there is mechanical drift, the unit won't be present for the entire session. This adds all sorts of problems for analysis down the road - how can you compare accorss sessions if your unit comes or goes between them? we therefore want to use only units that we can follow through the recording.\n",
    "\n",
    "Note that exactly which unit QC criteria you use may very based on the questions you ask. Have a question that doesn't really depend on well isolated neurons? Try loosening these criteria. Need to know about the differences of well-isolated units over a long timescale? try tightening them. As a starting place, though, these numbers are reasonable. If you want to get fancy, more information about QC critria can be found here: https://spikeinterface.readthedocs.io/en/stable/modules/qualitymetrics.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC criteria? \n",
    "good_units = units_electrode_table[\n",
    "    (units_electrode_table.isi_violations<.5) &\n",
    "    (units_electrode_table.amplitude_cutoff<.1) &\n",
    "    (units_electrode_table.presence_ratio>.95)\n",
    "    ]\n",
    "print(len(good_units))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "    \n",
    "<h3> How many 'good' VISp neurons do we have?? </h3>\n",
    "\n",
    "np.unique has a handy \"return_counts\" option. Try using it to count the good VISp neurons. Never used this command before? Try using \"shift+tab\" to see the help menu!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 1.7 Select an area to analyis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_structure_units_table  = good_units[good_units.location == 'VISp']\n",
    "len(this_structure_units_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "# 2 Can we decode stimulus identity from a single neuron?\n",
    "\n",
    "Its time to look at our first neuron of the day! (WOOT!)\n",
    "\n",
    "A very important comment as you get ready to start your projects: In this course, we will fit many models with increasing levels of abstraction to our data. It is always going to be tempting to jump straight into modeling, and gloss over the actual data. Don't be tempted to do this! Looking at and understanding your  data is, at the end of the day, alway more important than whatever model you might fit. After all, if your data are bad, so is your model.\n",
    "\n",
    "Fortunately, someone here has already done a good chunk of the quality control on our data. So here, \"data\" really means the spike times for a given unit.\n",
    "\n",
    "With this in mind, lets look at the activity of a single neuron as the mouse is presented with its stimulus. First, we need some data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a neuron to start.\n",
    "unit = 6\n",
    "# Get the spike times for this neuron\n",
    "spike_times = this_structure_units_table.spike_times.values[unit]\n",
    "print(spike_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and get the times that the stimulus presentation started\n",
    "stim_times=  active_stimuli.start_time.values\n",
    "print(stim_times[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 2.1 Rasters and PSTHs\n",
    "\n",
    "What we what to know right now is, how does this neuron respond to a stimulus?\n",
    "\n",
    "To answer this, we need to grab a window of time around the when the stimulus was presented. We will then look at the only the spikes that happen within this window. \n",
    "\n",
    "We are going to make two types of plots to visualize these data. \n",
    "\n",
    "+ The first we call a <i> raster </i> plot. Here, we will represent each spike by a dot, and each trial as a row in our plot.\n",
    "+ The raster plot is useful for visualizing activity across trials, but it can be difficult to quantify. With this in mind, we will also make a <i> Peristimulus Time Histogram </i> or <i> PSTH </i>. This is the averaged stimulus triggered average for the neuron or, equivalently, a histogram showing the average of the raster plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a stimulus window.\n",
    "pre_window  = .2 # How far before the stimulus should we look?\n",
    "post_window = .75 # How far after the stimulus should we look?\n",
    "bin_size = .01 # What size bins do we want for our PSTH?\n",
    "bins = np.arange(-pre_window,post_window+bin_size,bin_size) # Set up bins\n",
    "bin_centers = bins[:-1]+bin_size/2\n",
    "# Storage for data.\n",
    "triggered_spike_times = []\n",
    "triggered_stim_index = []\n",
    "\n",
    "# Loop through the stimuli!!\n",
    "for i, stim_time in enumerate(stim_times):\n",
    "    # Select spikes that fall within the time window around this stimulus\n",
    "    mask = ((spike_times >= stim_time - pre_window) & \n",
    "            (spike_times < stim_time + post_window))\n",
    "    \n",
    "    # Align spike times to stimulus onset (0 = stimulus)\n",
    "    trial_spikes = spike_times[mask] - stim_time\n",
    "\n",
    "    triggered_spike_times.append(trial_spikes)\n",
    "    triggered_stim_index.append(np.ones(len(trial_spikes))*i)\n",
    "\n",
    "# triggered_spike_times now has the times of each spike per trial.  \n",
    "print(triggered_spike_times[:3])\n",
    "# triggered_trial_index is for keeping track of which trial this spike was from\n",
    "print(triggered_stim_index[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting, we are going to want to concatenate these data into one big vector\n",
    "triggered_spike_times = np.concatenate(triggered_spike_times)\n",
    "triggered_stim_index = np.concatenate(triggered_stim_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a plot\n",
    "fig,ax = plt.subplots(nrows = 2,figsize =(5,10))\n",
    "\n",
    "# Plot the raster! Its just dots, so we use scatter.\n",
    "# The 'k' here is a shoutout to all the matlab users...\n",
    "ax[0].scatter(triggered_spike_times,triggered_stim_index,s = 1,c = 'k')\n",
    "ax[0].set_xlabel('Time from stimulus (seconds)')\n",
    "ax[0].set_ylabel('Stim number (sorted)')\n",
    "ax[0].axvline([0],c = 'r')\n",
    "\n",
    "# and make the histogram.\n",
    "a,b = np.histogram(triggered_spike_times,bins = bins)\n",
    "# Divide by # of trials, then bin size to get a rate estimate in Spikes/Sec = Hz\n",
    "a = a/np.max(triggered_stim_index)/bin_size\n",
    "ax[1].plot(bin_centers,a,c = 'k')\n",
    "ax[1].set_xlabel('Time from stimulus (seconds)')\n",
    "ax[1].set_ylabel('Spike Rate (Hz)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Great! We have a neuron that responds to the stimulus!\n",
    "\n",
    "But remember our goal here- we want to know if we can decode the difference between stimuli from this neuron. With that in mind, lets try seperating out the different stimulus identities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, figsize=(5, 10))\n",
    "\n",
    "# Define a stimulus window.\n",
    "pre_window  = .2 # How far before the stimulus should we look?\n",
    "post_window = .75 # How far after the stimulus should we look?\n",
    "bin_size = .01 # What size bins do we want for our PSTH?\n",
    "bins = np.arange(-pre_window,post_window+bin_size,bin_size) # Set up bins\n",
    "bin_centers = bins[:-1]+bin_size/2\n",
    "\n",
    "n_trials = len(stim_id)\n",
    "\n",
    "# Ensure integer trial ids for indexing\n",
    "triggered_stim_index = triggered_stim_index.astype(int, copy=False)\n",
    "\n",
    "# Use unique to compute how many of each type of trial there are \n",
    "_,counts  = np.unique(stim_id,return_counts=True)\n",
    "\n",
    "# Blocks will be offset by the number of trials of each type\n",
    "offsets = np.zeros(len(unq_stim) + 1, dtype=int)\n",
    "offsets[1:] = np.cumsum(counts)\n",
    "\n",
    "# We are remapping trials to rows, and we need a place to store the output\n",
    "trial_to_row = np.empty(n_trials, dtype=int)\n",
    "\n",
    "\n",
    "# Loop through trial types\n",
    "for i, stim in enumerate(unq_stim):\n",
    "    \n",
    "    this_trials = np.flatnonzero(stim_id == i)  # trial ids for stim i\n",
    "    trial_to_row[this_trials] = offsets[i] + np.arange(len(this_trials), dtype=int)\n",
    "\n",
    "    if len(this_trials) == 0:\n",
    "        continue\n",
    "\n",
    "    mask = np.isin(triggered_stim_index, this_trials)\n",
    "    this_times = triggered_spike_times[mask]\n",
    "    this_rows  = trial_to_row[triggered_stim_index[mask]]  # OK now that tsi is int\n",
    "\n",
    "    # Plot this chunk of the raster\n",
    "    ax[0].scatter(this_times, this_rows, s=.1)  # add marker='|' for look\n",
    "\n",
    "    # Plot this PSTH\n",
    "    a, _ = np.histogram(this_times, bins=bins)\n",
    "    rate = a / this_trials.size / bin_size\n",
    "    ax[1].plot(bin_centers, rate, label=str(stim))\n",
    "\n",
    "\n",
    "# Labels\n",
    "ax[0].set_xlabel('Time from stimulus (seconds)')\n",
    "ax[0].set_ylabel('Trial # (sorted)')\n",
    "ax[1].set_xlabel('Time from stimulus (seconds)')\n",
    "ax[1].set_ylabel('Spike Rate (Hz)')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 2.2 Linear Classifier\n",
    "So far these data are promising - on average, the neuron doesn't respond the same way on every trial.\n",
    "\n",
    "We are now ready to use a mathematical model to see if we decode stimulus identity from this neuron. Note that, despite these promising averages, this many not be a guarantee - the variability across trials may still make this very difficult.\n",
    "\n",
    "For now, we will start with a linear classifier. Specifically, we will use <b>sklearn</b>'s implementation of a <i>Support Vector Classifier</i>, or <i>SVC</i>. These are part of a broader class of algorithms known as a <i> Support Vector Machine (SVM) </i>. Importantly, this means we have officially made it to the machine learning part of the course. Yay!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "A linear classifier will, in effect, attempt to find a linear divider to separate to different classes of data. \n",
    "\n",
    "A SVC is a type of <i> supervised </i> classifier. This means that it is trained to do classification using data that have known labels.\n",
    "\n",
    "Before we get to the neural data, lets work through a simple example of what this all means with some fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some randomly generated data\n",
    "x_1 = np.random.normal(loc = 1,scale = 5,size = 10000)\n",
    "x_2 = np.random.normal(loc = 6,scale = 5,size = 10000)\n",
    "# Class identities\n",
    "y_1 = np.ones(x_1.shape)\n",
    "y_2 = np.ones(x_2.shape)*2\n",
    "\n",
    "# make these into one big vector\n",
    "x = np.concatenate([x_1,x_2])\n",
    "y = np.concatenate([y_1,y_2])\n",
    "\n",
    "# Plot them!\n",
    "fig,ax = plt.subplots()\n",
    "tmp_bins = np.arange(-10,20,.15)\n",
    "ax.hist(x_1,tmp_bins,color= 'teal')\n",
    "ax.hist(x_2,tmp_bins,color = 'darkorange')\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('# Samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "One important thing to note here is that we are passing labeled data in an effort to learn the distinction between classes. This is a great way to build a model, but it comes with risks- because we are training our model using these data, it would not be fair to evaluate model performance using the same data.\n",
    "\n",
    "In cases where we don't want to go out and colect more data, we can split our data into \"training\" and \"testing.\" <b>Sklearn</b> provides a handy function for doing this called \"train_test_spilt.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Now we are ready to fit our model. Again, <b>sklearn</b> provides a useful interface for this.\n",
    "\n",
    "Note: In this tutorial we are using SVC. However, one of the nice things about <b>sklearn</b> is that that it uses a standardized interface for all of its model fits. This makes it very easy to play with different classifiers, model types, etc.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model fitting object\n",
    "svc = LinearSVC()\n",
    "svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC requires inputs be a certain shape. \n",
    "# When using 1-d arrays, we need to some reshaping to follow this convention\n",
    "svc.fit(x_train.reshape(-1, 1),y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Now that we have this model, we can use it to predict new data. One way of doing this is to our held out data, and see how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction = svc.predict(x_test.reshape(-1, 1))\n",
    "score = np.sum(y_prediction==y_test)/(len(y_test))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can also be acomplished using the built in \"score\" function\n",
    "svc.score(x_test.reshape(-1, 1),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Alternatively, we can pass in x values to find out what class would have been predicted. This can be very useful for understanding how our model is actually doing its classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers to test\n",
    "tmp_bins = np.arange(-10,20.15,.15)\n",
    "\n",
    "# Plot our origional distributions\n",
    "fig,ax = plt.subplots(nrows = 2)\n",
    "ax[0].hist(x_1,tmp_bins,color= 'teal')\n",
    "ax[0].hist(x_2,tmp_bins,color = 'darkorange')\n",
    "ax[0].set_xlabel('Value')\n",
    "ax[0].set_ylabel('# Samples')\n",
    "\n",
    "# \n",
    "bins_prediction = svc.predict(tmp_bins.reshape(-1,1))\n",
    "ax[1].axhline(1,color=  'teal',linewidth = 20,alpha = .6)\n",
    "ax[1].axhline(2,color= 'darkorange',linewidth = 20,alpha = .6)\n",
    "ax[1].plot(tmp_bins,bins_prediction,'k')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 2.3 Now lets try with our neuron!!\n",
    "\n",
    "The challenge, of course, is that our neural data is high dimensional. To keep things interpretable, lets try to small window from the overall spike train that will give us a decent chance of decoding information from this neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows = 2,figsize =(5,10))\n",
    "\n",
    "# A counter, useful for stacking plots\n",
    "counter = 0\n",
    "for ii in range(len(unq_stim)):\n",
    "    # spike times for this trial type\n",
    "    this_triggered_spike_times = triggered_spike_times[stim_id[triggered_stim_index.astype(int)]==ii]\n",
    "\n",
    "    # trial index subselected by this trial type.\n",
    "    this_trl_idx = np.arange(np.sum(stim_id[triggered_stim_index.astype(int)]==ii))\n",
    "\n",
    "    ax[0].scatter(this_triggered_spike_times,counter + this_trl_idx,s = 1)\n",
    "\n",
    "\n",
    "    # stack the plots\n",
    "    counter += np.max(this_trl_idx)\n",
    "    \n",
    "    # Plot the raster just for this stimulus type\n",
    "    a,b = np.histogram(this_triggered_spike_times,bins = bins)\n",
    "    a = a/np.max(this_trl_idx)/bin_size\n",
    "    ax[1].plot(bin_centers,a,label = unq_stim[ii]) # Note that we are labeling each plot\n",
    "    ax[1].set_xlabel('Time from stimulus (seconds)')\n",
    "    ax[1].set_ylabel('Spike Rate (Hz)')\n",
    "\n",
    "ax[0].axvspan(xmin = 0,xmax = .35,color = 'Gray',alpha = .2)\n",
    "ax[0].set_xlabel('Time from stimulus (seconds)')\n",
    "ax[0].set_ylabel('Trial number (sorted)')\n",
    "ax[1].axvspan(xmin = 0,xmax = .35,color = 'Gray',alpha = .2)\n",
    "ax[1].legend() # Plot a legend using the established labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "We can start by counting the number of spikes for each trial type within this window. We will then go about fitting a linear classier to attempt to decode trial type from number of spikes within our window!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times = this_structure_units_table.spike_times.values[unit]\n",
    "stim_times = active_stimuli.start_time\n",
    "\n",
    "start= 0\n",
    "stop = .350\n",
    "\n",
    "spike_count = []\n",
    "trial_index = []\n",
    "\n",
    "for i, stim_time in enumerate(stim_times):\n",
    "    # Select spikes that fall within the time window around this stimulus\n",
    "    mask = ((spike_times >= stim_time + start) & \n",
    "            (spike_times < stim_time + stop))\n",
    "    \n",
    "    # Count spikes in this bin\n",
    "    spike_count.append(len(spike_times[mask]))\n",
    "    \n",
    "spike_count = np.array(spike_count)\n",
    "trial_index = np.arange(len(spike_count))\n",
    "trial_id_types,trial_id = np.unique(active_stimuli.image_name.values,return_inverse=  True)\n",
    "\n",
    "# Get information for manual control of colors\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "color_list = [entry['color'] for entry in prop_cycle]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "for jj in range(9):\n",
    "    ax.scatter(spike_count[trial_id==jj],np.random.random(len(spike_count[trial_id==jj]))/2+jj-.25,s = 1)\n",
    "    box = ax.boxplot(spike_count[trial_id==jj],\n",
    "                        positions = [jj],widths=[.75],\n",
    "                        showfliers=False,vert= False,\n",
    "                        patch_artist = True,\n",
    "                        medianprops=dict(color=\"black\", linewidth=2))\n",
    "    box['boxes'][0].set_facecolor(color_list[jj])\n",
    "    box['boxes'][0].set_alpha(.3)\n",
    "\n",
    "ax.set_yticklabels(unq_stim,rotation = 0)\n",
    "ax.set_xlabel('# of spikes')\n",
    "ax.set_ylabel('Stimulus')\n",
    "ax.set_title('Spike count by trial type')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just as before, we need to split out data.\n",
    "x_train,x_test,y_train,y_test = train_test_split(spike_count,trial_id)\n",
    "\n",
    "# Note that we are using the same x = predictor, y = class \n",
    "# label convention that we were using before. \n",
    "print(f'x train shape: {x_train.shape}')\n",
    "print(f'y train shape: {y_train.shape}')\n",
    "print(f'x test shape: {x_test.shape}')\n",
    "print(f'y test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "    \n",
    "<h3> Ready? </h3>\n",
    "Go ahead and fit a linear SCV to the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new model, call it svc\n",
    "svc = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "\n",
    "<h3> Try passing a few spike rates to your shiny new model </h3> \n",
    "\n",
    "How do things compare to the histogram we just made?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "color_list = [entry['color'] for entry in prop_cycle]\n",
    "\n",
    "input_spike_count = np.arange(0,np.max(spike_count),.1) \n",
    "predicted_class = svc.predict(input_spike_count.reshape(-1,1))\n",
    "fig,ax = plt.subplots(nrows = 2)\n",
    "\n",
    "\n",
    "for jj in range(9):\n",
    "    ax[0].scatter(spike_count[trial_id==jj],np.random.random(len(spike_count[trial_id==jj]))/2+jj-.25,s = 1)\n",
    "    box = ax[0].boxplot(spike_count[trial_id==jj],\n",
    "                        positions = [jj],widths=[.75],\n",
    "                        showfliers=False,vert= False,\n",
    "                        patch_artist = True,\n",
    "                        medianprops=dict(color=\"black\", linewidth=2))\n",
    "    box['boxes'][0].set_facecolor(color_list[jj])\n",
    "    box['boxes'][0].set_alpha(.3)\n",
    "\n",
    "\n",
    "ax[0].set_yticklabels(unq_stim,rotation = 0)\n",
    "ax[0].set_xlabel('# of spikes')\n",
    "ax[0].set_ylabel('Stimulus')\n",
    "ax[0].set_title('Spike count by trial type')\n",
    "ax[0].legend()\n",
    "\n",
    "for ii in range(len(unq_stim)):\n",
    "    ax[1].axhline(ii,c = color_list[ii],linewidth=10,alpha = .5)\n",
    "ax[1].plot(input_spike_count,predicted_class,'k',linewidth = 4)\n",
    "ax[1].set_yticks(np.arange(9))\n",
    "ax[1].set_yticklabels(unq_stim,rotation = 0)\n",
    "ax[1].set_xlabel('# of spikes')\n",
    "ax[1].set_ylabel('predicted class')\n",
    "ax[1].set_title('Prediction by spike count')\n",
    "\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "But how good is this model, really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "\n",
    "<h3> Compute the model score on your held out testing data</h3> \n",
    "\n",
    "Is this good? Do you think its better than \"chance\"?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions for our held out test data\n",
    "prediction = svc.score(x_test.reshape(-1,1),y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model score\n",
    "score = svc.score(x_test.reshape(-1,1),y_test.ravel())\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Is this any good? When examining the answer to that question, it is probably most useful to ask, \"how much better was our classifier than if we had just guessed by chance\".\n",
    "\n",
    "In cases where you have evenly distributed classes, chance estimates are easy. For example, if we just had 8 evenly distributed classes, chance performance would be 1/8. \n",
    "\n",
    "However, we don't have exactly even numbers of trials: there are, for example, fewer omission trials. The mouse also spent more time struggling with some stimuli than others. In cases like this, we can sometimes estimate a \"chance\" figure by asking, \"if we shuffled trial identities, what fraction of the time would they line up with the true trial identity.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chance_est = []\n",
    "for ii in range(1000):\n",
    "    shuffled_trial_id = trial_id[np.random.randint(len(trial_id))]\n",
    "    chance_est.append(np.sum(trial_id==shuffled_trial_id)/len(trial_id))\n",
    "\n",
    "chance  =np.mean(chance_est)\n",
    "print(f'Chance estimate: {chance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "It is worth saying that, because we have more than one class, we can dig a little deeper into our prediction.\n",
    "\n",
    "Specifically, when we guess incorrectly, miss-classifications often have structure to them. A confusion matrix is a useful tool to understand mistakes a classifier makes. Here we plot, given a \"true\" class, what was the distribution of predictions made by our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "prediction = svc.predict(x_test.reshape(-1,1))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "im  = ax.imshow(confusion_matrix(y_true=y_test, y_pred=prediction, normalize='pred'))\n",
    "ax.set_xlabel('Predicted Class')\n",
    "ax.set_ylabel('True Class')\n",
    "ax.set_xticks(np.arange(len(unq_stim)))\n",
    "ax.set_yticks(np.arange(len(unq_stim)))\n",
    "ax.set_xticklabels(unq_stim,rotation = 90)\n",
    "ax.set_yticklabels(unq_stim)\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Fraction Guessed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "\n",
    "<h3> Compare the confusion matrix to the histogram for this neuron </h3> \n",
    "\n",
    "Can you understand why the classifier is making the errors that it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "# 3.0 Now for a neural population.\n",
    "\n",
    "While the mathematical formalism is useful to understand what our chosen neuron is doing here, if we are being honest its probably overkill for understanding this single neurons activity. There are, in fact, whole classes of regression models that might be better suited for asking what information a neuron encodes. \n",
    "\n",
    "Why, then, have we spent so much time on this example? Decoding gives us a mathematical way to look at what information we can extract from a neural population. As this population becomes larger, however, it can become increasingly difficult to visualize and intuit what our decoder is doing under the hood. Starting with this one-dimension example will help with intuition as we move to this harder case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "With that said, lets use decoding to take a hard look at the timescale of VISPs' encoding of stimulus identity. \n",
    "\n",
    "Doing this is going to require a fair bit of data wrangling. Just as before, we needed to get the number of spikes per trial. But if we are looking over time and over neurons, we need to get the number of spikes for each neuron on each trial. \n",
    "\n",
    "Here is one way to do this, though you will notice that it is a little on the slow side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "n_neurons = len(this_structure_units_table.spike_times.values)\n",
    "stim_times = active_stimuli.start_time\n",
    "\n",
    "\n",
    "bins = np.arange(-.2,.5,.05)\n",
    "storage = np.empty((n_neurons,len(stim_times),len(bins)-1))\n",
    "\n",
    "for nn in range(n_neurons):\n",
    "    spike_times = this_structure_units_table.spike_times.values[nn]\n",
    "\n",
    "    spike_count = []\n",
    "    trial_index = []\n",
    "    \n",
    "    for i, stim_time in enumerate(stim_times):\n",
    "        # Select spikes that fall within the time window around this stimulus\n",
    "\n",
    "        mask = ((spike_times >= stim_time + np.min(bins)) & \n",
    "                (spike_times < stim_time + np.max(bins)))\n",
    "        \n",
    "        # Align spike times to stimulus onset (0 = stimulus)\n",
    "        trial_spikes,_ = np.histogram(spike_times[mask] - stim_time,bins)\n",
    "        \n",
    "        spike_count.append(trial_spikes)\n",
    "    \n",
    "    storage[nn,:,:] = np.array(spike_count)\n",
    "trial_index = np.arange(len(spike_count))\n",
    "trial_id_types,trial_id = np.unique(active_stimuli.image_name.values,return_inverse=  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "This code is actually pretty hard to follow. It would be more readable if we converted the core piece of it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binned_triggered_spike_times(spike_times,stim_times,bins):\n",
    "    spike_count = []\n",
    "    trial_index = []\n",
    "    \n",
    "    for i, stim_time in enumerate(stim_times):\n",
    "        # Select spikes that fall within the time window around this stimulus\n",
    "        \n",
    "        mask = ((spike_times >= stim_time + np.min(bins)) & \n",
    "                (spike_times < stim_time + np.max(bins)))\n",
    "        \n",
    "        # Align spike times to stimulus onset (0 = stimulus)\n",
    "        trial_spikes,_ = np.histogram(spike_times[mask] - stim_time,bins)\n",
    "        \n",
    "        spike_count.append(trial_spikes)\n",
    "    return np.array(spike_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "You will notice, though, that this is a nice way to organize our code, but it won't actually make anything go any faster.\n",
    "\n",
    "Now, we are going to give you the *fast* version of this function. If you end up working with triggered spike trains much during this course, the difference in time here can be pretty meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binned_triggered_spike_counts_fast(spike_times, stim_times, bins):\n",
    "    \"\"\"\n",
    "    Fast peri-stimulus time histogram using searchsorted.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spike_times : 1D array_like, sorted\n",
    "        Times of all spikes (e.g. in seconds).\n",
    "    stim_times : 1D array_like\n",
    "        Times of stimulus onsets.\n",
    "    bins : 1D array_like\n",
    "        Bin edges *relative* to stimulus (e.g. np.linspace(-0.1, 0.5, 61)).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counts : 2D ndarray, shape (n_trials, len(bins)-1)\n",
    "        counts[i, j] is the number of spikes in bin j of trial i.\n",
    "    \"\"\"\n",
    "    # ensure numpy arrays\n",
    "    spike_times = np.asarray(spike_times)\n",
    "    stim_times = np.asarray(stim_times)\n",
    "    bins = np.asarray(bins)\n",
    "\n",
    "    # If your spike_times isn't already sorted, uncomment:\n",
    "    # spike_times = np.sort(spike_times)\n",
    "\n",
    "    n_trials = stim_times.size\n",
    "    n_bins = bins.size - 1\n",
    "    counts = np.zeros((n_trials, n_bins), dtype=int)\n",
    "\n",
    "    for i, stim in enumerate(stim_times):\n",
    "        # compute the absolute edges for this trial\n",
    "        edges = stim + bins\n",
    "        # find the insertion indices for each edge\n",
    "        idx = np.searchsorted(spike_times, edges, side='left')\n",
    "        # differences between successive indices = counts per bin\n",
    "        counts[i, :] = np.diff(idx)\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "n_neurons = len(this_structure_units_table.spike_times.values)\n",
    "stim_times = active_stimuli.start_time\n",
    "\n",
    "\n",
    "bins = np.arange(-.2,.5,.1)\n",
    "storage = np.empty((n_neurons,len(stim_times),len(bins)-1))\n",
    "\n",
    "for nn in range(n_neurons):\n",
    "    spike_times = this_structure_units_table.spike_times.values[nn]\n",
    "\n",
    "    spike_count = []\n",
    "    trial_index = []\n",
    "\n",
    "    storage[nn,:,:]  = get_binned_triggered_spike_counts_fast(spike_times,stim_times,bins)\n",
    "\n",
    "trial_index = np.arange(len(spike_count))\n",
    "trial_id_types,trial_id = np.unique(stimuli.image_name.values,return_inverse=  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "The %%timeit is preventing the above code from saving inputs, so lets run this fast version one more time for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons = len(this_structure_units_table.spike_times.values)\n",
    "stim_times = active_stimuli.start_time\n",
    "\n",
    "\n",
    "bins = np.arange(-.2,.75,.1)\n",
    "storage = np.empty((n_neurons,len(stim_times),len(bins)-1))\n",
    "\n",
    "for nn in range(n_neurons):\n",
    "    spike_times = this_structure_units_table.spike_times.values[nn]\n",
    "\n",
    "    spike_count = []\n",
    "    trial_index = []\n",
    "\n",
    "    storage[nn,:,:]  = get_binned_triggered_spike_counts_fast(spike_times,stim_times,bins)\n",
    "\n",
    "trial_index = np.arange(len(spike_count))\n",
    "trial_id_types,trial_id = np.unique(active_stimuli.image_name.values,return_inverse=  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "\n",
    "Now, even though we binned at a higher temporal rate than before, we can still fit a classifier that is analogous to our single-neuron version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_change_number = active_stimuli.flashes_since_change.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_number = 0\n",
    "\n",
    "# Select a time window to use for decoding\n",
    "inc_time_idx = np.where((bins>.1) & (bins<.3))[0] # select times to include\n",
    "start_idx = np.min(inc_time_idx)\n",
    "end_idx = np.max(inc_time_idx)\n",
    "\n",
    "# Find the number of spikes in the selected window\n",
    "X = np.sum(storage[:,stimulus_change_number==change_number,start_idx:end_idx],axis=2).T \n",
    "# And the trial identity for each of the selected stimuli\n",
    "y = trial_id[stimulus_change_number==change_number]\n",
    "\n",
    "# Spit the data\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y)\n",
    "\n",
    "# Fit the model\n",
    "svc = LinearSVC()\n",
    "svc.fit(x_train,y_train)\n",
    "print(f'Model score: {svc.score(x_test,y_test)}')\n",
    "\n",
    "# make and plot the confusion matrix\n",
    "prediction = svc.predict(x_test)\n",
    "fig,ax = plt.subplots()\n",
    "im  = ax.imshow(confusion_matrix(y_true=y_test, y_pred=prediction, normalize='pred'))\n",
    "ax.set_xlabel('Predicted Class')\n",
    "ax.set_ylabel('True Class')\n",
    "ax.set_xticks(np.arange(len(unq_stim)))\n",
    "ax.set_yticks(np.arange(len(unq_stim)))\n",
    "ax.set_xticklabels(unq_stim,rotation = 90)\n",
    "ax.set_yticklabels(unq_stim)\n",
    "\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label('Fraction Guessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### 3.1 Timescale of population dynamics  \n",
    "\n",
    "What this means is that we can, using all of our VISp neurons, <b>very</b> reliably decode stimulus identity. This is, in and of itself, pretty exciting. But we can use our new decoding technique to dig deeper into the behavior of the neural population within our chosen time window.\n",
    "\n",
    "To do this, we can loop through a fit a model to each time bin. Once we have that model, we can compute the score for each model using both the training and testing partitions in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_train = np.zeros(len(bins)-1)\n",
    "score_test = np.zeros(len(bins)-1) \n",
    "\n",
    "# Loop through and fit a model to each time bin.\n",
    "for ii in range(len(bins)-1):\n",
    "    svc = LinearSVC()\n",
    "\n",
    "    # Find the number of spikes in the selected window\n",
    "    X = storage[:,stimulus_change_number==change_number,ii].T\n",
    "    # And the trial identity for each of the selected stimuli\n",
    "    y = trial_id[stimulus_change_number==change_number]\n",
    "        \n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,y)\n",
    "    svc.fit(x_train,y_train)\n",
    "    # Find score on both the training and the test data.\n",
    "    score_train[ii] = svc.score(x_train,y_train)\n",
    "    score_test[ii] = svc.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Plot the score on the test data.\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(bins[:-1]+.05,score_test,label = 'Test')\n",
    "ax.axhline(np.mean(chance),linestyle = '--',label = 'Chance est.',c = 'r')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Time from stimulus')\n",
    "ax.set_ylabel('Model Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Remember how we made such a big deal about splitting our data before? The importance of doing this becomes especially visible now that we have moved to the high dimensional, many neuron case. Because the dimensionality here is so high, models tend to do very, very well at predicting the data they were trained on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "\n",
    "<h3> Plot the testing and training data scores </h3> \n",
    "\n",
    "How does it compare? How does it compare to our chance estimate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "    \n",
    "### 3.2 Cross validation\n",
    "\n",
    "So far, we have seen only a single train/test split for each dataset. However, it is often useful to split your data multiple times. Depending on how you do this, it can allow you to do more fine grained characterization of either the variability or (depending on how you split) time course of what you are decoding.\n",
    "\n",
    "To make this easy, <b>sklearn</b> includes a cross_validate function that automates much of what you could achieve yourself using a for loop. Its default is to do 5-fold cross validation. This means that the data is split in 5ths. 5 models are then fit, each with 4/5 of the data for training and the remaining 1/5 for testing. \n",
    "\n",
    "Lets try it here for 0-100 ms.\n",
    "\n",
    "Note: There is nothing special about the number 5- you can use n-fold cross validation as suits your question. In the extreme case, you can use \"Leave-one-out\" cross validation, where n= number of samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(LinearSVC(),storage[:,:,3].T,trial_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "Now, we can loop through to get a sense of our decoder variance over time. This effectivly allows us to draw errorbars on our decoder!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cross = 5\n",
    "scores = np.empty([len(bins)-1,n_cross])       \n",
    "\n",
    "for ii in range(len(bins)-1):\n",
    "    # Find the number of spikes in the selected window\n",
    "    X = storage[:,stimulus_change_number==change_number,ii].T\n",
    "    # And the trial identity for each of the selected stimuli\n",
    "    y = trial_id[stimulus_change_number==change_number]\n",
    "    \n",
    "    scores[ii,:] = cross_validate(LinearSVC(),X,y,cv = n_cross)['test_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "median_score = []\n",
    "for ii in range(len(bins)-1):\n",
    "    px = plt.scatter([bins[ii]]*(n_cross),scores[ii,:],c = [1,2,3,4,5])\n",
    "    median_score.append(np.mean(scores[ii,:]))\n",
    "plt.plot(bins[:-1]+bin_size/2,median_score)\n",
    "ax.axhline(np.mean(chance_est),linestyle = '--',label = 'Chance est.',c = 'r')\n",
    "ax.set_xlabel('Time from stimulus')\n",
    "ax.set_ylabel('Model Score')\n",
    "cbar = plt.colorbar(px)\n",
    "cbar.set_ticks([1,2,3,4,5])\n",
    "cbar.set_label('Fold #')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### A couple quick notes: \n",
    "### 3.4 firing rates\n",
    "Lets quickly look at the distribution of maximum firing rates for our VISp units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = storage[:,:,3].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots(nrows = 2)\n",
    "ax[0].hist(np.max(rates,axis=0),25)\n",
    "ax[0].set_xlabel('Max Rate')\n",
    "ax[0].set_ylabel('# of units')\n",
    "ax[0].set_title('Max')\n",
    "\n",
    "ax[1].hist(np.mean(rates,axis=0),25)\n",
    "ax[1].set_xlabel('Mean Rate')\n",
    "ax[1].set_ylabel('# of units')\n",
    "ax[1].set_title('Mean')\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "There is a danger here: depending on how you do you decoding, the high firing rate neurons could end up caring far more import than the low rate neurons.\n",
    "\n",
    "A common solution to (1) center our data around zero by subtracting the mean and (2) normalize the variance by dividing by the standard deviation. In other words, we need to \"Z-score\" our data or \"Standardize\" it. \n",
    "\n",
    "$ \\vec{x_{rescaled}} = \\frac{\\vec{x}-mean(x)}{stdev(x)}$\n",
    "\n",
    "<b>Sklearn</b> provides a handy interface to do this using the \"<b>StandardScaler</b>.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = StandardScaler()\n",
    "rates_rescaled = S.fit_transform(rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows = 2)\n",
    "ax[0].hist(np.max(rates_rescaled,axis=0),25)\n",
    "ax[0].set_xlabel('Max Rate')\n",
    "ax[0].set_ylabel('# of units')\n",
    "ax[0].set_title('Max')\n",
    "\n",
    "ax[1].hist(np.mean(rates_rescaled,axis=0),25)\n",
    "ax[1].set_xlabel('Mean Rate')\n",
    "ax[1].set_ylabel('# of units')\n",
    "ax[1].set_title('Mean')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "#### BUT WAIT!\n",
    "\n",
    "If this scaling was such a risk, why have we made it this far into the workshop without worrying about it?\n",
    "\n",
    "It turns out that linear models are one of the cases where rescaling will not change behavior of our model. Rescaling will, of course, change the weights fit by the linear model. But, because Z-scoring is itself a linear operation, it wont impact the loss function or prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = cross_validate(LinearSVC(),rates,stim_id)\n",
    "print(mdl['test_score'])\n",
    "\n",
    "mdl_rescaled = cross_validate(LinearSVC(),rates_rescaled,stim_id)\n",
    "print(mdl_rescaled['test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "#### So...Should I rescale?\n",
    "\n",
    "It depends. In many cases (like here!), it won't matter. \n",
    "\n",
    "However, there are cases where it can make a big difference. Rescaling is, for example, an essential part of principle components analysis- without it large values would dominate the resulting PCs.\n",
    "\n",
    "Even if rescaling won't change the predictions of your model, it can make it easier to understand. Model weights will scale with firing rate. As a result, if you want to compare the impacts of different units with different weight on your linear model, rescaling can make these weights interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "### A few quick notes: \n",
    "### 3.5 Population size impacts decoding\n",
    "\n",
    "Up till now, we seen decoding with both a single neuron, and our entire V1 population.\n",
    "\n",
    "As it turns out, there is a lot of space in the middle of these two questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The random library is useful for sub selecting data\n",
    "import random\n",
    "\n",
    "# Here is an example of sampling, without replacement, from a set of indicies.\n",
    "idx_list = list(np.arange(0,len(this_structure_units_table)))\n",
    "samples = random.sample(idx_list,6)\n",
    "subset  = np.array(samples).astype(int)\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of neuron sets to grab\n",
    "n_selections = 25\n",
    "# number of neurons per set.\n",
    "n_neurons = np.arange(1,len(this_structure_units_table),1)\n",
    "\n",
    "# Storage for our model scores\n",
    "scores = np.zeros([n_selections,len(n_neurons)])\n",
    "\n",
    "# Choose the window 0-100ms after the stimulus\n",
    "X = storage[:,stimulus_change_number==change_number,3].T\n",
    "# And the trial identity for each of the selected stimuli\n",
    "y = trial_id[stimulus_change_number==change_number]\n",
    "\n",
    "# Loop through nuerons and subsets\n",
    "for nn,neuron_count in enumerate(n_neurons):\n",
    "    for ii in range(n_selections):\n",
    "        samples = random.sample(idx_list,neuron_count)\n",
    "        subset  = np.array(samples).astype(int)\n",
    "        scores[ii,nn] = np.mean(cross_validate(LinearSVC(),X[:,subset],y,cv = 3)['test_score'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the mean and standard error of the mean for each neuron number\n",
    "means = np.mean(scores,axis =0)\n",
    "st_err = np.std(scores,axis = 0)/np.sqrt(n_selections)\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.errorbar(n_neurons,\n",
    "             means,\n",
    "             yerr = st_err,\n",
    "           label = 'Model Score')\n",
    "ax.set_xlabel('# of neurons')\n",
    "ax.set_ylabel('Average Model Score')\n",
    "ax.axhline(chance,c = 'r',linestyle = '--',label = 'chance')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "# 4.0 Whats next?\n",
    "\n",
    "So far, we have seen that visual cortex encodes image identity in a visual task.\n",
    "\n",
    "This, in and of itself, may not be that surprising to you. But as you have seen, decoders are a useful tool in understanding the dynamics of a population - we have used them to understand the time course of visual responses, as well as to understand how broadly distributed the visual code might be.\n",
    "\n",
    "Now, lets try moving beyond image identity. This is, after all, a change detection task. Even though \"change\" images are drawn from the same set as the non-change images, can we differentiate change image presentations? \n",
    "\n",
    "To answer this, we can use the same \"storage\" matrix that we created before- its just a matter of changing 'y' class labels that we are decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #DFF0D8; \">\n",
    "\n",
    "<h3> Try decoding change images. </h3> \n",
    "\n",
    "Can you plot the timecourse of change encoding?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some code to get things started\n",
    "fig,ax = plt.subplots()\n",
    "median_score = np.zeros(len(bins)-1)\n",
    "for ii in range(len(bins)-1):\n",
    "    X = storage[:,:,ii].T\n",
    "    y = active_stimuli.is_change.values\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "<h3> How did you do? A little too well? </h3> \n",
    "\n",
    "Lets see if we can figure out what went wrong here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait...what why are we doing so well.\n",
    "prob_of_no_change = 1-np.sum(stimuli.is_change)/len(stimuli)\n",
    "print('Success rate if you always guessed no: ' + str(prob_of_no_change))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "In cases with very uneven sampling, the 'balanced' option for the Linear SVC can be very helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode somethings else.\n",
    "fig,ax = plt.subplots()\n",
    "median_score = []\n",
    "for ii in range(len(bins)-1):\n",
    "    scores = cross_validate(LinearSVC(class_weight='balanced'),storage[:,:,ii].T,active_stimuli.is_change.values,)\n",
    "    scores = scores['test_score']\n",
    "    ax.scatter([bins[ii]+np.median(np.diff(bins))/2]*n_cross,scores,c = [0,1,2,3,4])\n",
    "    median_score.append(np.median(scores))\n",
    "ax.plot(bins[:-1]+np.median(np.diff(bins))/2,median_score)\n",
    "ax.axhline(.5,linestyle = '--',c = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 3px solid #000; padding: 1px; padding-left: 10px; background: #F0FAFF; \">\n",
    "\n",
    "# 4.1 Closing thoughts.\n",
    "\n",
    "This workshop has focused on decoding and how to assess your decoding. Here, we largely used cross validation to assess how well a model was doing across nominally homogenous block of trials. But what we are really asking with cross validation is \"how well does my model do on data it hasn't seen before.\" \n",
    "\n",
    "As you saw in Shawn's talk this morning, however, mice and mouse behavior are often far from homogenous. \n",
    "\n",
    "It is often useful to fit a model in one condition and test in another- this is a way to assess if population encoding has changed. Hold on to this idea - we will see it again later this afternoon!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
